{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afd8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhg/anaconda3/envs/test/lib/python3.13/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tifffile import imread\n",
    "import cv2\n",
    "import skimage.io as sio\n",
    "\n",
    "import albumentations as A\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import MaskRCNN, FasterRCNN_ResNet50_FPN_Weights, MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.ops import box_convert\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8479fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "        self.train_coco_path = os.path.join(pathlib.Path(root_dir).parent, 'train_coco.json')\n",
    "        if not os.path.exists(self.train_coco_path):\n",
    "            self.generate_coco(self.train_coco_path)   \n",
    "        self.train_coco = COCO(self.train_coco_path)\n",
    "        self.num_classes = len(self.train_coco.loadCats(self.train_coco.getCatIds()))\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for img_dir in os.listdir(self.root):\n",
    "            tmp_dir = os.path.join(self.root, img_dir)\n",
    "\n",
    "            if not self.is_test:\n",
    "                img_path = os.path.join(tmp_dir, 'image.tif')\n",
    "                \n",
    "                mask_paths = [\n",
    "                    entry.name for entry in pathlib.Path(tmp_dir).iterdir()\n",
    "                    if entry.name.startswith(\"class\") and entry.is_file()\n",
    "                ]\n",
    "\n",
    "                samples.append({'image': img_path, 'masks': mask_paths})\n",
    "            else:\n",
    "                test_img_json_path = os.path.join(pathlib.Path(self.root).parent, 'test_image_name_to_ids.json')\n",
    "                with open(test_img_json_path, 'r') as f:\n",
    "                    samples = json.load(f)\n",
    "\n",
    "                # for idx in range(len(samples)):\n",
    "                #     samples[idx]['file_name'] = os.path.join(self.root, samples[idx]['file_name'])\n",
    "        return samples\n",
    "    \n",
    "    def mask_to_polygons(self, mask, epsilon=1.0):\n",
    "        contours,_ = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        polygons = []\n",
    "        for contour in contours:\n",
    "            if len(contour) > 2:\n",
    "                poly = contour.reshape(-1).tolist()\n",
    "                if len(poly) > 4: #Ensures valid polygon\n",
    "                    polygons.append(poly)\n",
    "        return polygons\n",
    "\n",
    "    def generate_coco(self, output_dir, train=True):\n",
    "        annotations = []\n",
    "        images = []\n",
    "        categories = []\n",
    "        all_labels = []\n",
    "        ann_id = 0\n",
    "        \n",
    "        for img_id, sample in enumerate(self.samples):\n",
    "            print(f'({img_id}/{len(self.samples)})')\n",
    "            img_path, mask_paths = sample['image'], sample['masks']\n",
    "            img = cv2.imread(img_path)\n",
    "            masks = [cv2.imread(os.path.join(pathlib.Path(img_path).parent, mask_path), cv2.IMREAD_UNCHANGED) for mask_path in mask_paths]\n",
    "\n",
    "            images.append({\n",
    "                \"id\": img_id,\n",
    "                \"file_name\": img_path,\n",
    "                \"height\": img.shape[0],\n",
    "                \"width\": img.shape[1]\n",
    "            })\n",
    "\n",
    "            for mask in masks:\n",
    "                unique_values = np.unique(mask)\n",
    "                all_labels.append(unique_values)\n",
    "                for value in unique_values:\n",
    "                    if value == 0:  # Ignore background\n",
    "                        continue\n",
    "\n",
    "                    object_mask = (mask == value).astype(np.uint8) * 255\n",
    "                    polygons = self.mask_to_polygons(object_mask)\n",
    "\n",
    "                    for poly in polygons:\n",
    "                        ann_id += 1\n",
    "                        annotations.append({\n",
    "                            \"id\": ann_id,\n",
    "                            \"image_id\": img_id,\n",
    "                            \"category_id\": 1,  # Only one category: Nuclei\n",
    "                            \"segmentation\": [poly],\n",
    "                            \"area\": cv2.contourArea(np.array(poly).reshape(-1, 2)),\n",
    "                            \"bbox\": list(cv2.boundingRect(np.array(poly).reshape(-1, 2))),\n",
    "                            \"iscrowd\": 0\n",
    "                        })\n",
    "\n",
    "        all_labels = np.unique(np.concatenate(all_labels).tolist())\n",
    "\n",
    "        for idx, label in enumerate(all_labels):\n",
    "            categories.append({\"id\": idx+1, \"name\": int(label)})\n",
    "\n",
    "        coco_input = {\n",
    "            \"images\": images,\n",
    "            \"annotations\": annotations,\n",
    "            \"categories\": categories\n",
    "        }\n",
    "\n",
    "        print(f'Saving train coco json')\n",
    "        # if train:\n",
    "        with open(os.path.join(output_dir, 'train_coco.json'), 'w') as f:\n",
    "            json.dump(coco_input, f)\n",
    "        # else:\n",
    "        #     with open(os.path.join(output_dir, 'instances_val2017.json'), 'w') as f:\n",
    "        #         json.dump(coco_input, f)\n",
    "                \n",
    "\n",
    "\n",
    "    # def pickout_label_in_mask(self, mask):\n",
    "    #     all_labels_in_mask = np.unique(mask)\n",
    "\n",
    "    #     res = {}\n",
    "\n",
    "    #     masks = []\n",
    "    #     labels = []\n",
    "    #     for label in all_labels_in_mask:\n",
    "    #         if label > self.num_classes:\n",
    "    #             self.num_classes = label\n",
    "    #         if label == 0:\n",
    "    #             continue\n",
    "                \n",
    "    #         tmp = np.zeros_like(mask)\n",
    "    #         tmp[mask == label] = 1\n",
    "    #         masks.append(tmp.astype(np.uint8))\n",
    "    #         labels.append(label)\n",
    "\n",
    "    #     res['mask'] = masks\n",
    "    #     res['label'] = labels\n",
    "        \n",
    "    #     return res\n",
    "    \n",
    "    \n",
    "    # def generate_target(self, sample):\n",
    "    #     target = {}\n",
    "\n",
    "    #     # for sample in samples:\n",
    "    #     image_path = sample['image']\n",
    "    #     mask_dict = sample['masks']\n",
    "\n",
    "    #     boxes = []\n",
    "    #     labels = []\n",
    "    #     masks = []\n",
    "    #     for mask in mask_dict:\n",
    "    #         mask_path = os.path.join(pathlib.Path(image_path).parent, mask)\n",
    "    #         mask_image = imread(mask_path)\n",
    "    #         mask_with_label = self.pickout_label_in_mask(mask_image)\n",
    "            \n",
    "    #         for i in range(len(mask_with_label['mask'])):\n",
    "    #             mask = mask_with_label['mask'][i]\n",
    "    #             category = mask_with_label['label'][i]\n",
    "                \n",
    "    #             if np.sum(mask) > 0:\n",
    "    #                 mask = mask.astype(np.uint8)\n",
    "    #                 contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #                 for contour in contours:\n",
    "    #                     if len(contour) > 2:\n",
    "    #                         masks.append(mask)\n",
    "    #                         labels.append(category)\n",
    "    #                         boxes.append(cv2.boundingRect(contour))\n",
    "\n",
    "    #     target['boxes'] = torch.tensor(np.array(boxes), dtype=torch.float32)\n",
    "    #     target['labels'] = torch.tensor(np.array(labels), dtype=torch.int64)\n",
    "    #     target['masks'] = torch.tensor(np.array(masks), dtype=torch.uint8)\n",
    "\n",
    "    #     return target\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     sample = self.samples[idx]\n",
    "\n",
    "    #     if not self.is_test:\n",
    "    #         image = torch.tensor(cv2.imread(sample['image']), dtype=torch.float32).permute(2, 0, 1)#.astype(np.float32) / 255.0\n",
    "    #         target = self.generate_target(sample)\n",
    "            \n",
    "    #         target['boxes'] = box_convert(target['boxes'], in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "    #         return self.transform(image), target\n",
    "    #     else:\n",
    "\n",
    "    #         return 1, 0\n",
    "\n",
    "    def poly2mask(self, segmentation, img_size):\n",
    "        \"\"\"\n",
    "        多邊形標註轉二值掩碼\n",
    "        :param segmentation: COCO格式的多邊形坐標列表 [[x1,y1,x2,y2,...]]\n",
    "        :param img_size: 目標圖像尺寸 (height, width)\n",
    "        \"\"\"\n",
    "        # 自動檢測標註類型\n",
    "        if isinstance(segmentation, dict):\n",
    "            # 處理RLE格式\n",
    "            return coco_mask.decode(segmentation)\n",
    "        else:\n",
    "            # 處理多邊形格式\n",
    "            rle = coco_mask.frPyObjects(segmentation, img_size[0], img_size[1])\n",
    "            return coco_mask.decode(rle)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "\n",
    "        if not self.is_test:\n",
    "            img_ids = self.train_coco.getImgIds(imgIds=index)\n",
    "            img_info = self.train_coco.loadImgs(img_ids)\n",
    "            # image = cv2.imread(img_info[0]['file_name']) / 255.0\n",
    "            image = Image.open(img_info[0]['file_name']).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "            img_size = [img_info[0]['height'], img_info[0]['width']]\n",
    "\n",
    "            \n",
    "            boxes = []\n",
    "            masks = []\n",
    "            labels = []\n",
    "            ann_ids = self.train_coco.getAnnIds(imgIds=img_ids)\n",
    "            annotations = self.train_coco.loadAnns(ann_ids)\n",
    "            for ann in annotations:\n",
    "                boxes.append(ann['bbox'])\n",
    "                masks.append(self.poly2mask(ann['segmentation'], img_size).squeeze())\n",
    "                labels.append(ann[\"category_id\"])\n",
    "\n",
    "            boxes = self.resize_box(boxes, img_size, target_size=[224,224])\n",
    "            boxes = box_convert(torch.tensor(boxes, dtype=torch.float32), in_fmt='xywh', out_fmt='xyxy')\n",
    "            target = {'boxes': torch.as_tensor(boxes, dtype=torch.float32), \n",
    "                      'masks': torch.as_tensor(np.array(masks), dtype=torch.bool), \n",
    "                      'labels': torch.as_tensor(np.array(labels), dtype=torch.int64)}\n",
    "            \n",
    "            return image, target\n",
    "        else:\n",
    "\n",
    "            return 1, 0\n",
    "        \n",
    "    def resize_box(self, boxes, orig_size, target_size):\n",
    "        # Eat xywh\n",
    "        scale_w = target_size[1] / orig_size[1]\n",
    "        scale_h = target_size[0] / orig_size[0]\n",
    "\n",
    "        for box in boxes:\n",
    "            box[0] *= scale_w  # x\n",
    "            box[1] *= scale_h  # y\n",
    "            box[2] *= scale_w  # w\n",
    "            box[3] *= scale_h  # h\n",
    "\n",
    "        return boxes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d4d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = '..'\n",
    "train_dir = os.path.join(project_root, 'dataset/train')\n",
    "test_dir = os.path.join(project_root, 'dataset/test_release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a321fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_coco_path = f'/home/bhg/visual_dl/lab3/dataset'\n",
    "# train_set = MedicalDataset(root_dir=train_dir)\n",
    "# train_set.generate_coco(train_coco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e557f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "box: torch.Size([465, 4])\n",
      "mask: torch.Size([465, 1678, 1463])\n",
      "label: torch.Size([465])\n",
      "tensor([33.9904, 15.7521, 39.0431, 20.4243])\n",
      "tensor([[[0.8593, 0.8293, 0.7810,  ..., 0.6892, 0.9255, 0.9362],\n",
      "         [0.8492, 0.8139, 0.7655,  ..., 0.5991, 0.8944, 0.9379],\n",
      "         [0.8168, 0.7807, 0.7381,  ..., 0.4569, 0.7979, 0.9353],\n",
      "         ...,\n",
      "         [0.8211, 0.7692, 0.7464,  ..., 0.8486, 0.8181, 0.7398],\n",
      "         [0.7771, 0.7405, 0.6968,  ..., 0.8194, 0.7878, 0.7201],\n",
      "         [0.8033, 0.7169, 0.6576,  ..., 0.8301, 0.7613, 0.7097]],\n",
      "\n",
      "        [[0.7858, 0.7444, 0.6521,  ..., 0.5869, 0.9122, 0.9202],\n",
      "         [0.7594, 0.7073, 0.6348,  ..., 0.4325, 0.8488, 0.9219],\n",
      "         [0.7136, 0.6686, 0.5983,  ..., 0.2560, 0.7195, 0.9152],\n",
      "         ...,\n",
      "         [0.7280, 0.6560, 0.6105,  ..., 0.7509, 0.7031, 0.5902],\n",
      "         [0.6557, 0.6028, 0.5438,  ..., 0.7143, 0.6678, 0.5672],\n",
      "         [0.6795, 0.5601, 0.4940,  ..., 0.7316, 0.6148, 0.5394]],\n",
      "\n",
      "        [[0.8793, 0.8710, 0.8510,  ..., 0.6789, 0.9470, 0.9477],\n",
      "         [0.8737, 0.8556, 0.8534,  ..., 0.5252, 0.9008, 0.9420],\n",
      "         [0.8620, 0.8574, 0.8435,  ..., 0.3545, 0.7966, 0.9525],\n",
      "         ...,\n",
      "         [0.8938, 0.8395, 0.8313,  ..., 0.8842, 0.8505, 0.7796],\n",
      "         [0.8661, 0.8351, 0.8521,  ..., 0.8568, 0.8310, 0.7841],\n",
      "         [0.8747, 0.8413, 0.8441,  ..., 0.8683, 0.7986, 0.7802]]]) torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "train_transform=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(size=[224,224], antialias=True),\n",
    "    # T.CenterCrop(size=224),\n",
    "    # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "train_set = MedicalDataset(root_dir=train_dir, transform=train_transform)\n",
    "img, target = train_set[1]\n",
    "print(f'box: {target['boxes'].shape}')\n",
    "print(f'mask: {target['masks'].shape}')\n",
    "print(f'label: {target['labels'].shape}')\n",
    "print(target['boxes'][0])\n",
    "\n",
    "print(img, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a354976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train_transform = A.Compose([\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.VerticalFlip(p=0.3),\n",
    "#     A.Rotate(limit=15, p=0.4),\n",
    "#     A.CLAHE(p=0.5),\n",
    "#     A.GridDistortion(p=0.2),\n",
    "#     A.RandomBrightnessContrast(p=0.3)\n",
    "# ], additional_targets={'mask': 'mask'})\n",
    "train_transform=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(size=[224, 224], antialias=True),\n",
    "    # T.CenterCrop(size=224),\n",
    "    # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_set = MedicalDataset(root_dir=train_dir, transform=train_transform)\n",
    "# test_set = MedicalDataset(root_dir=test_dir, is_test=True)\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "        targets.append({\n",
    "            'boxes': target['boxes'],\n",
    "            'labels': target['labels'],\n",
    "            'masks': target['masks']\n",
    "        })\n",
    "    \n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory='cuda', collate_fn=custom_collate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e358b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m optimizer.zero_grad()\n\u001b[32m     38\u001b[39m losses.backward()\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m bar.set_postfix(loss=losses.detach().cpu().item())\n\u001b[32m     42\u001b[39m bar.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/optim/optimizer.py:504\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    501\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/optim/sgd.py:126\u001b[39m, in \u001b[36mSGD.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    120\u001b[39m momentum_buffer_list: \u001b[38;5;28mlist\u001b[39m[Optional[Tensor]] = []\n\u001b[32m    122\u001b[39m has_sparse_grad = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    123\u001b[39m     group, params, grads, momentum_buffer_list\n\u001b[32m    124\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmomentum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdampening\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnesterov\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mmomentum\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[32m0\u001b[39m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, momentum_buffer_list):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/optim/sgd.py:301\u001b[39m, in \u001b[36msgd\u001b[39m\u001b[34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    299\u001b[39m     func = _single_tensor_sgd\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/optim/sgd.py:436\u001b[39m, in \u001b[36m_multi_tensor_sgd\u001b[39m\u001b[34m(params, grads, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[39m\n\u001b[32m    433\u001b[39m         bufs.append(cast(Tensor, device_momentum_buffer_list[i]))\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_states_with_momentum_buffer:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     torch._foreach_add_(bufs, device_grads, alpha=\u001b[32m1\u001b[39m - dampening)\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "model.to('cuda')\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "num_epochs = 10\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    print(f'a')\n",
    "    model.train()\n",
    "    bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for images, targets in bar:\n",
    "        print('b')\n",
    "        images = [image.to('cuda') for image in images]\n",
    "        targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
    "        # import torch.nn.functional as F\n",
    "        # images = F.interpolate(\n",
    "        #     images,\n",
    "        #     size=224,\n",
    "        #     mode='bilinear',\n",
    "        #     align_corners=False\n",
    "        # )\n",
    "        \n",
    "        # targets['masks'] = F.interpolate(\n",
    "        #     targets['masks'],\n",
    "        #     size=224,\n",
    "        #     mode='nearest'\n",
    "        # )\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bar.set_postfix(loss=losses.detach().cpu().item())\n",
    "        bar.update()\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {losses.item()}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/53 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_convert\n\u001b[32m      9\u001b[39m bar = tqdm(train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mInference\u001b[39m\u001b[33m\"\u001b[39m, total=\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:735\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    733\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    738\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    741\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1493\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1490\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1493\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1496\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1455\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1452\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1454\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1455\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1456\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1457\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1286\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1274\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1275\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1283\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1288\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1290\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1291\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/queues.py:111\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    110\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/connection.py:1148\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/selectors.py:398\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    396\u001b[39m ready = []\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "model = maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "bar = tqdm(train_loader, desc=\"Inference\", total=len(train_loader))\n",
    "for images, targets in bar:\n",
    "    images = list(image.to('cuda') for image in images)\n",
    "    targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        masks = prediction['masks'].cpu().numpy()\n",
    "        labels = prediction['labels'].cpu().numpy()\n",
    "        \n",
    "        # Process the predictions as needed\n",
    "        print(f\"Image {i}:\")\n",
    "        print(\"Boxes:\", boxes)\n",
    "        print(\"Masks:\", masks)\n",
    "        print(\"Labels:\", labels)\n",
    "    \n",
    "    bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.get_model(\n",
    "        args.model, weights=args.weights, weights_backbone=args.weights_backbone, num_classes=num_classes, **kwargs\n",
    "    )\n",
    "model.roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=len(class_names),bias=True)\n",
    "model.roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=len(class_names)*4,bias=True)\n",
    "model.roi_heads.mask_predictor.mask_fcn_logits = nn.Conv2d(256, len(class_names),kernel_size=(1,1),stride=(1,1))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMaskRCNN(MaskRCNN):\n",
    "    def __init__(self, backbone, num_classes=None, **kwargs):\n",
    "        super().__init__(backbone, num_classes, **kwargs)\n",
    "        # 添加邊界感知分支\n",
    "        self.boundary_head = self._build_boundary_head()\n",
    "        \n",
    "    def _build_boundary_head(self):\n",
    "        layers = [\n",
    "            torch.nn.Conv2d(256, 256, 3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(256, 256, 3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(256, 1, 1)  # 邊界檢測輸出\n",
    "        ]\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # 原始輸出\n",
    "        outputs = super().forward(images, targets)\n",
    "        \n",
    "        # 邊界檢測分支\n",
    "        if self.training and targets is not None:\n",
    "            boundary_maps = self.boundary_head(outputs['features'])\n",
    "            outputs['boundary_loss'] = self.compute_boundary_loss(boundary_maps, targets)\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "def create_model(num_classes=5, pretrained=True):\n",
    "    backbone = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    # backbone = torchvision.models._utils.IntermediateLayerGetter(\n",
    "    #     backbone, return_layers={'layer4': 'out'}\n",
    "    # )\n",
    "    # backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    \n",
    "    model = EnhancedMaskRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        min_size=512,\n",
    "        max_size=512\n",
    "    )\n",
    "    \n",
    "    # 修改分類頭\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # 修改mask頭\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask, 256, num_classes\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df76213",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m SGD(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 35\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(num_classes, pretrained)\u001b[0m\n\u001b[1;32m     29\u001b[0m backbone \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mresnet50(weights\u001b[38;5;241m=\u001b[39mResNet50_Weights\u001b[38;5;241m.\u001b[39mDEFAULT)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# backbone = torchvision.models._utils.IntermediateLayerGetter(\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     backbone, return_layers={'layer4': 'out'}\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEnhancedMaskRCNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# min_size=512,\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_size=512\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 修改分類頭\u001b[39;00m\n\u001b[1;32m     43\u001b[0m in_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor\u001b[38;5;241m.\u001b[39mcls_score\u001b[38;5;241m.\u001b[39min_features\n",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m, in \u001b[0;36mEnhancedMaskRCNN.__init__\u001b[0;34m(self, backbone, num_classes, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, backbone, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# 添加邊界感知分支\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboundary_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_boundary_head()\n",
      "File \u001b[0;32m~/anaconda3/envs/vdl-lab3/lib/python3.10/site-packages/torchvision/models/detection/mask_rcnn.py:215\u001b[0m, in \u001b[0;36mMaskRCNN.__init__\u001b[0;34m(self, backbone, num_classes, min_size, max_size, image_mean, image_std, rpn_anchor_generator, rpn_head, rpn_pre_nms_top_n_train, rpn_pre_nms_top_n_test, rpn_post_nms_top_n_train, rpn_post_nms_top_n_test, rpn_nms_thresh, rpn_fg_iou_thresh, rpn_bg_iou_thresh, rpn_batch_size_per_image, rpn_positive_fraction, rpn_score_thresh, box_roi_pool, box_head, box_predictor, box_score_thresh, box_nms_thresh, box_detections_per_img, box_fg_iou_thresh, box_bg_iou_thresh, box_batch_size_per_image, box_positive_fraction, bbox_reg_weights, mask_roi_pool, mask_head, mask_predictor, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes should be None when mask_predictor is specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m out_channels \u001b[38;5;241m=\u001b[39m \u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask_roi_pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     mask_roi_pool \u001b[38;5;241m=\u001b[39m MultiScaleRoIAlign(featmap_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m], output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, sampling_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vdl-lab3/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'out_channels'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = create_model().to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = SGD(params, lr=0.003, momentum=0.9, weight_decay=0.0005)\n",
    "lr_sched = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# 混合損失函數\n",
    "def hybrid_loss(pred_masks, gt_masks, boundaries):\n",
    "    mask_loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_masks, gt_masks)\n",
    "    boundary_loss = torch.nn.functional.mse_loss(pred_masks, boundaries)\n",
    "    return mask_loss + 0.3 * boundary_loss\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, targets in train_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{'masks': t.to(device)} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "    \n",
    "    lr_sched.step()\n",
    "    print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_to_coco(results, image_ids):\n",
    "    coco_results = []\n",
    "    for img_id, output in zip(image_ids, results):\n",
    "        for score, mask, label in zip(output['scores'], output['masks'], output['labels']):\n",
    "            rle = binary_mask_to_rle(mask)\n",
    "            coco_results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": label.item(),\n",
    "                \"segmentation\": rle,\n",
    "                \"score\": score.item()\n",
    "            })\n",
    "    return coco_results\n",
    "\n",
    "def binary_mask_to_rle(mask):\n",
    "    # RLE編碼實現\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return {'size': list(mask.shape[-2:]), 'counts': runs.tolist()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06683b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loader = DataLoader(test_set, batch_size=2, shuffle=False)\n",
    "\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(batch.to(device))\n",
    "        results.extend(outputs)\n",
    "\n",
    "# 生成最終提交文件\n",
    "with open('test-results.json', 'w') as f:\n",
    "    json.dump(masks_to_coco(results, test_set.image_ids), f)\n",
    "\n",
    "print(\"Submission file generated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
