{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "8afd8226",
      "metadata": {
        "id": "8afd8226"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tifffile import imread\n",
        "import cv2\n",
        "import skimage.io as sio\n",
        "\n",
        "import albumentations as A\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection import MaskRCNN, FasterRCNN_ResNet50_FPN_Weights, MaskRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from torchvision.ops import box_convert\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "\n",
        "import pathlib\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as coco_mask\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/file/d/1B0qWNzQZQmfQP7x7o4FDdgb9GvPDoFzI/view --fuzzy\n",
        "!mkdir ../dataset\n",
        "!tar -xzf hw3-data-release.tar.gz\n",
        "!mv test_release/ ../dataset\n",
        "!mv train/ ../dataset/\n",
        "!mv test_image_name_to_ids.json ../dataset/"
      ],
      "metadata": {
        "id": "5tzWQPzcCgKR",
        "outputId": "ad512dc1-afb2-4162-c283-0b073259ace9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5tzWQPzcCgKR",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1B0qWNzQZQmfQP7x7o4FDdgb9GvPDoFzI\n",
            "From (redirected): https://drive.google.com/uc?id=1B0qWNzQZQmfQP7x7o4FDdgb9GvPDoFzI&confirm=t&uuid=4a711dcd-b1c0-492c-a48b-21b997382f72\n",
            "To: /content/hw3-data-release.tar.gz\n",
            "100% 387M/387M [00:06<00:00, 62.3MB/s]\n",
            "mkdir: cannot create directory ‘../dataset’: File exists\n",
            "mv: cannot move 'test_release/' to '../dataset/test_release': Directory not empty\n",
            "mv: cannot move 'train/' to '../dataset/train': Directory not empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "8479fbf0",
      "metadata": {
        "id": "8479fbf0"
      },
      "outputs": [],
      "source": [
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root = root_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "        self.train_coco_path = os.path.join(pathlib.Path(root_dir).parent, 'train_coco.json')\n",
        "        if not os.path.exists(self.train_coco_path):\n",
        "            self.generate_coco(self.train_coco_path)\n",
        "        self.train_coco = COCO(self.train_coco_path)\n",
        "        self.num_classes = len(self.train_coco.loadCats(self.train_coco.getCatIds()))\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for img_dir in os.listdir(self.root):\n",
        "            tmp_dir = os.path.join(self.root, img_dir)\n",
        "\n",
        "            if not self.is_test:\n",
        "                img_path = os.path.join(tmp_dir, 'image.tif')\n",
        "\n",
        "                mask_paths = [\n",
        "                    entry.name for entry in pathlib.Path(tmp_dir).iterdir()\n",
        "                    if entry.name.startswith(\"class\") and entry.is_file()\n",
        "                ]\n",
        "\n",
        "                samples.append({'image': img_path, 'masks': mask_paths})\n",
        "            else:\n",
        "                test_img_json_path = os.path.join(pathlib.Path(self.root).parent, 'test_image_name_to_ids.json')\n",
        "                with open(test_img_json_path, 'r') as f:\n",
        "                    samples = json.load(f)\n",
        "\n",
        "                # for idx in range(len(samples)):\n",
        "                #     samples[idx]['file_name'] = os.path.join(self.root, samples[idx]['file_name'])\n",
        "        return samples\n",
        "\n",
        "    def mask_to_polygons(self, mask, epsilon=1.0):\n",
        "        contours,_ = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "        polygons = []\n",
        "        for contour in contours:\n",
        "            if len(contour) > 2:\n",
        "                poly = contour.reshape(-1).tolist()\n",
        "                if len(poly) > 4: #Ensures valid polygon\n",
        "                    polygons.append(poly)\n",
        "        return polygons\n",
        "\n",
        "    def generate_coco(self, output_dir, train=True):\n",
        "        annotations = []\n",
        "        images = []\n",
        "        categories = []\n",
        "        all_labels = []\n",
        "        ann_id = 0\n",
        "\n",
        "        for img_id, sample in enumerate(self.samples):\n",
        "            print(f'({img_id}/{len(self.samples)})')\n",
        "            img_path, mask_paths = sample['image'], sample['masks']\n",
        "            img = cv2.imread(img_path)\n",
        "            masks = [cv2.imread(os.path.join(pathlib.Path(img_path).parent, mask_path), cv2.IMREAD_UNCHANGED) for mask_path in mask_paths]\n",
        "\n",
        "            images.append({\n",
        "                \"id\": img_id,\n",
        "                \"file_name\": img_path,\n",
        "                \"height\": img.shape[0],\n",
        "                \"width\": img.shape[1]\n",
        "            })\n",
        "\n",
        "            for mask in masks:\n",
        "                unique_values = np.unique(mask)\n",
        "                all_labels.append(unique_values)\n",
        "                for value in unique_values:\n",
        "                    if value == 0:  # Ignore background\n",
        "                        continue\n",
        "\n",
        "                    object_mask = (mask == value).astype(np.uint8) * 255\n",
        "                    polygons = self.mask_to_polygons(object_mask)\n",
        "\n",
        "                    for poly in polygons:\n",
        "                        ann_id += 1\n",
        "                        annotations.append({\n",
        "                            \"id\": ann_id,\n",
        "                            \"image_id\": img_id,\n",
        "                            \"category_id\": 1,  # Only one category: Nuclei\n",
        "                            \"segmentation\": [poly],\n",
        "                            \"area\": cv2.contourArea(np.array(poly).reshape(-1, 2)),\n",
        "                            \"bbox\": list(cv2.boundingRect(np.array(poly).reshape(-1, 2))),\n",
        "                            \"iscrowd\": 0\n",
        "                        })\n",
        "\n",
        "        all_labels = np.unique(np.concatenate(all_labels).tolist())\n",
        "\n",
        "        for idx, label in enumerate(all_labels):\n",
        "            categories.append({\"id\": idx+1, \"name\": int(label)})\n",
        "\n",
        "        coco_input = {\n",
        "            \"images\": images,\n",
        "            \"annotations\": annotations,\n",
        "            \"categories\": categories\n",
        "        }\n",
        "\n",
        "        print(f'Saving train coco json')\n",
        "        # if train:\n",
        "        with open(output_dir, 'w') as f:\n",
        "            json.dump(coco_input, f)\n",
        "        # else:\n",
        "        #     with open(os.path.join(output_dir, 'instances_val2017.json'), 'w') as f:\n",
        "        #         json.dump(coco_input, f)\n",
        "\n",
        "\n",
        "\n",
        "    # def pickout_label_in_mask(self, mask):\n",
        "    #     all_labels_in_mask = np.unique(mask)\n",
        "\n",
        "    #     res = {}\n",
        "\n",
        "    #     masks = []\n",
        "    #     labels = []\n",
        "    #     for label in all_labels_in_mask:\n",
        "    #         if label > self.num_classes:\n",
        "    #             self.num_classes = label\n",
        "    #         if label == 0:\n",
        "    #             continue\n",
        "\n",
        "    #         tmp = np.zeros_like(mask)\n",
        "    #         tmp[mask == label] = 1\n",
        "    #         masks.append(tmp.astype(np.uint8))\n",
        "    #         labels.append(label)\n",
        "\n",
        "    #     res['mask'] = masks\n",
        "    #     res['label'] = labels\n",
        "\n",
        "    #     return res\n",
        "\n",
        "\n",
        "    # def generate_target(self, sample):\n",
        "    #     target = {}\n",
        "\n",
        "    #     # for sample in samples:\n",
        "    #     image_path = sample['image']\n",
        "    #     mask_dict = sample['masks']\n",
        "\n",
        "    #     boxes = []\n",
        "    #     labels = []\n",
        "    #     masks = []\n",
        "    #     for mask in mask_dict:\n",
        "    #         mask_path = os.path.join(pathlib.Path(image_path).parent, mask)\n",
        "    #         mask_image = imread(mask_path)\n",
        "    #         mask_with_label = self.pickout_label_in_mask(mask_image)\n",
        "\n",
        "    #         for i in range(len(mask_with_label['mask'])):\n",
        "    #             mask = mask_with_label['mask'][i]\n",
        "    #             category = mask_with_label['label'][i]\n",
        "\n",
        "    #             if np.sum(mask) > 0:\n",
        "    #                 mask = mask.astype(np.uint8)\n",
        "    #                 contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    #                 for contour in contours:\n",
        "    #                     if len(contour) > 2:\n",
        "    #                         masks.append(mask)\n",
        "    #                         labels.append(category)\n",
        "    #                         boxes.append(cv2.boundingRect(contour))\n",
        "\n",
        "    #     target['boxes'] = torch.tensor(np.array(boxes), dtype=torch.float32)\n",
        "    #     target['labels'] = torch.tensor(np.array(labels), dtype=torch.int64)\n",
        "    #     target['masks'] = torch.tensor(np.array(masks), dtype=torch.uint8)\n",
        "\n",
        "    #     return target\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     sample = self.samples[idx]\n",
        "\n",
        "    #     if not self.is_test:\n",
        "    #         image = torch.tensor(cv2.imread(sample['image']), dtype=torch.float32).permute(2, 0, 1)#.astype(np.float32) / 255.0\n",
        "    #         target = self.generate_target(sample)\n",
        "\n",
        "    #         target['boxes'] = box_convert(target['boxes'], in_fmt='xywh', out_fmt='xyxy')\n",
        "\n",
        "    #         return self.transform(image), target\n",
        "    #     else:\n",
        "\n",
        "    #         return 1, 0\n",
        "\n",
        "    def poly2mask(self, segmentation, img_size):\n",
        "        \"\"\"\n",
        "        多邊形標註轉二值掩碼\n",
        "        :param segmentation: COCO格式的多邊形坐標列表 [[x1,y1,x2,y2,...]]\n",
        "        :param img_size: 目標圖像尺寸 (height, width)\n",
        "        \"\"\"\n",
        "        # 自動檢測標註類型\n",
        "        if isinstance(segmentation, dict):\n",
        "            # 處理RLE格式\n",
        "            return coco_mask.decode(segmentation)\n",
        "        else:\n",
        "            # 處理多邊形格式\n",
        "            rle = coco_mask.frPyObjects(segmentation, img_size[0], img_size[1])\n",
        "            return coco_mask.decode(rle)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.samples[index]\n",
        "\n",
        "        if not self.is_test:\n",
        "            img_ids = self.train_coco.getImgIds(imgIds=index)\n",
        "            img_info = self.train_coco.loadImgs(img_ids)\n",
        "            # image = cv2.imread(img_info[0]['file_name']) / 255.0\n",
        "            image = Image.open(img_info[0]['file_name']).convert(\"RGB\")\n",
        "            image = self.transform(image)\n",
        "            img_size = [img_info[0]['height'], img_info[0]['width']]\n",
        "\n",
        "\n",
        "            boxes = []\n",
        "            masks = []\n",
        "            labels = []\n",
        "            ann_ids = self.train_coco.getAnnIds(imgIds=img_ids)\n",
        "            annotations = self.train_coco.loadAnns(ann_ids)\n",
        "            for ann in annotations:\n",
        "                boxes.append(ann['bbox'])\n",
        "                tmp_mask = self.poly2mask(ann['segmentation'], img_size).squeeze()\n",
        "                # mask_ = F.interpolate(\n",
        "                #     tmp_mask,\n",
        "                #     size=(224, 224),\n",
        "                #     mode='nearest-exact'  # PyTorch 1.10+ 專用選項\n",
        "                # )\n",
        "                mask_ = cv2.resize(\n",
        "                    tmp_mask,\n",
        "                    (224, 224),\n",
        "                    interpolation=cv2.INTER_NEAREST_EXACT  # 精確最近鄰算法\n",
        "                )\n",
        "                masks.append(mask_)\n",
        "                labels.append(ann[\"category_id\"])\n",
        "\n",
        "            boxes = self.resize_box(boxes, img_size, target_size=[224,224])\n",
        "            boxes = box_convert(torch.tensor(boxes, dtype=torch.float32), in_fmt='xywh', out_fmt='xyxy')\n",
        "            masks = torch.as_tensor(np.array(masks), dtype=torch.bool)\n",
        "\n",
        "            target = {'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
        "                      'masks': masks,\n",
        "                      'labels': torch.as_tensor(np.array(labels), dtype=torch.int64)}\n",
        "\n",
        "            return image, target\n",
        "        else:\n",
        "\n",
        "            return 1, 0\n",
        "\n",
        "    def resize_box(self, boxes, orig_size, target_size):\n",
        "        # Eat xywh\n",
        "        scale_w = target_size[1] / orig_size[1]\n",
        "        scale_h = target_size[0] / orig_size[0]\n",
        "\n",
        "        for box in boxes:\n",
        "            box[0] *= scale_w  # x\n",
        "            box[1] *= scale_h  # y\n",
        "            box[2] *= scale_w  # w\n",
        "            box[3] *= scale_h  # h\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d0d4d02e",
      "metadata": {
        "id": "d0d4d02e"
      },
      "outputs": [],
      "source": [
        "project_root = '..'\n",
        "train_dir = os.path.join(project_root, 'dataset/train')\n",
        "test_dir = os.path.join(project_root, 'dataset/test_release')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9a321fc7",
      "metadata": {
        "id": "9a321fc7"
      },
      "outputs": [],
      "source": [
        "# train_coco_path = f'/home/bhg/visual_dl/lab3/dataset'\n",
        "# train_set = MedicalDataset(root_dir=train_dir)\n",
        "# train_set.generate_coco(train_coco_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "52e557f9",
      "metadata": {
        "id": "52e557f9",
        "outputId": "c4d73bc0-afe8-4588-fb90-db85ed50c88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.70s)\n",
            "creating index...\n",
            "index created!\n",
            "box: torch.Size([86, 4])\n",
            "mask: torch.Size([86, 224, 224])\n",
            "label: torch.Size([86])\n",
            "tensor([ 16.8142, 104.8320,  35.6234, 147.3920])\n",
            "tensor([[[0.7283, 0.5805, 0.5480,  ..., 0.5469, 0.5975, 0.5980],\n",
            "         [0.7332, 0.6403, 0.5805,  ..., 0.5863, 0.6309, 0.6200],\n",
            "         [0.7627, 0.6954, 0.6048,  ..., 0.6524, 0.6922, 0.5968],\n",
            "         ...,\n",
            "         [0.3114, 0.2937, 0.2715,  ..., 0.3510, 0.4343, 0.6885],\n",
            "         [0.3014, 0.2804, 0.2598,  ..., 0.3649, 0.4413, 0.6135],\n",
            "         [0.2940, 0.2680, 0.3035,  ..., 0.3818, 0.5199, 0.6588]],\n",
            "\n",
            "        [[0.6085, 0.4161, 0.3751,  ..., 0.3781, 0.4364, 0.4375],\n",
            "         [0.5894, 0.4423, 0.3612,  ..., 0.4151, 0.4654, 0.4284],\n",
            "         [0.6923, 0.4871, 0.3765,  ..., 0.4883, 0.5478, 0.4395],\n",
            "         ...,\n",
            "         [0.2613, 0.2613, 0.2400,  ..., 0.1632, 0.2396, 0.5874],\n",
            "         [0.2671, 0.2413, 0.2507,  ..., 0.1689, 0.2120, 0.4515],\n",
            "         [0.2613, 0.2429, 0.2717,  ..., 0.1901, 0.3071, 0.5305]],\n",
            "\n",
            "        [[0.8607, 0.7725, 0.6705,  ..., 0.7621, 0.7876, 0.7759],\n",
            "         [0.8317, 0.7360, 0.6158,  ..., 0.7750, 0.7961, 0.7427],\n",
            "         [0.8975, 0.7103, 0.5913,  ..., 0.8206, 0.8766, 0.7282],\n",
            "         ...,\n",
            "         [0.7885, 0.8366, 0.7764,  ..., 0.3474, 0.4152, 0.8077],\n",
            "         [0.7883, 0.7876, 0.7567,  ..., 0.3529, 0.3913, 0.6973],\n",
            "         [0.8425, 0.7936, 0.7131,  ..., 0.3829, 0.4990, 0.7604]]]) torch.Size([3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "train_transform=T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize(size=[224,224], antialias=True),\n",
        "    # T.CenterCrop(size=224),\n",
        "    # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "train_set = MedicalDataset(root_dir=train_dir, transform=train_transform)\n",
        "img, target = train_set[1]\n",
        "print(f\"box: {target['boxes'].shape}\")\n",
        "print(f\"mask: {target['masks'].shape}\")\n",
        "print(f\"label: {target['labels'].shape}\")\n",
        "print(target['boxes'][0])\n",
        "\n",
        "print(img, img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0a354976",
      "metadata": {
        "id": "0a354976",
        "outputId": "55100b1c-5f7b-4929-bfb7-cf4ce1db87ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.71s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "# train_transform = A.Compose([\n",
        "#     A.HorizontalFlip(p=0.5),\n",
        "#     A.VerticalFlip(p=0.3),\n",
        "#     A.Rotate(limit=15, p=0.4),\n",
        "#     A.CLAHE(p=0.5),\n",
        "#     A.GridDistortion(p=0.2),\n",
        "#     A.RandomBrightnessContrast(p=0.3)\n",
        "# ], additional_targets={'mask': 'mask'})\n",
        "train_transform=T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize(size=[224, 224], antialias=True),\n",
        "    # T.CenterCrop(size=224),\n",
        "    # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_set = MedicalDataset(root_dir=train_dir, transform=train_transform)\n",
        "# test_set = MedicalDataset(root_dir=test_dir, is_test=True)\n",
        "\n",
        "# def custom_collate(batch):\n",
        "#     images = []\n",
        "#     targets = []\n",
        "\n",
        "#     for img, target in batch:\n",
        "#         images.append(img)\n",
        "#         targets.append({\n",
        "#             'boxes': target['boxes'],\n",
        "#             'labels': target['labels'],\n",
        "#             'masks': target['masks']\n",
        "#         })\n",
        "\n",
        "#     images = torch.stack(images, dim=0)\n",
        "#     return images, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAX_BOXES_PER_BATCH = 5  # 依GPU記憶體調整\n",
        "\n",
        "# def custom_collate(batch):\n",
        "#     images = []\n",
        "#     targets = []\n",
        "#     residual_boxes = []  # 暫存未處理的boxes\n",
        "\n",
        "#     for img, target in batch:\n",
        "#         # 合併殘留框與新框\n",
        "#         all_boxes = residual_boxes + target['boxes']\n",
        "\n",
        "#         # 切割可用區塊\n",
        "#         use_boxes = all_boxes[:MAX_BOXES_PER_BATCH]\n",
        "#         residual_boxes = all_boxes[MAX_BOXES_PER_BATCH:]\n",
        "\n",
        "#         # 僅在有可用框時加入當前批次\n",
        "#         if len(use_boxes) > 0:\n",
        "#             images.append(img)\n",
        "#             targets.append({\n",
        "#                 'boxes': use_boxes,\n",
        "#                 'labels': target['labels'][:len(use_boxes)],\n",
        "#                 'masks': target['masks'][:len(use_boxes)]\n",
        "#             })\n",
        "\n",
        "#     # 遞歸處理殘留框\n",
        "#     if residual_boxes:\n",
        "#         dummy_img = torch.zeros_like(img)  # 填充虛擬數據\n",
        "#         return custom_collate([(dummy_img, {'boxes': residual_boxes})] + batch)\n",
        "\n",
        "#     images = torch.stack(images)\n",
        "#     return images, targets\n",
        "def custom_collate(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    for img, target in batch:\n",
        "        images.append(img)\n",
        "        targets.append({\n",
        "            'boxes': target['boxes'],\n",
        "            'labels': target['labels'],\n",
        "            'masks': target['masks']\n",
        "        })\n",
        "\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=False, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "93m5-5g8IUGS"
      },
      "id": "93m5-5g8IUGS",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e358b9b3",
      "metadata": {
        "id": "e358b9b3",
        "outputId": "34d952fa-4cd1-4bc4-b26c-219ae4ad0356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 1/10:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1/10:   0%|          | 0/105 [00:01<?, ?it/s, loss=7.72]\u001b[A\n",
            "Epoch 1/10:   1%|          | 1/105 [00:01<02:28,  1.42s/it, loss=7.72]\u001b[A\n",
            "Epoch 1/10:   2%|▏         | 2/105 [00:02<02:26,  1.42s/it, loss=5.8] \u001b[A\n",
            "Epoch 1/10:   3%|▎         | 3/105 [00:02<01:01,  1.66it/s, loss=5.8]\u001b[A\n",
            "Epoch 1/10:   4%|▍         | 4/105 [00:02<01:00,  1.66it/s, loss=4.47]\u001b[A\n",
            "Epoch 1/10:   5%|▍         | 5/105 [00:02<00:37,  2.65it/s, loss=4.47]\u001b[A\n",
            "Epoch 1/10:   5%|▍         | 5/105 [00:04<00:37,  2.65it/s, loss=7.06]\u001b[A\n",
            "Epoch 1/10:   6%|▌         | 6/105 [00:04<01:32,  1.08it/s, loss=7.06]\u001b[A\n",
            "Epoch 1/10:   7%|▋         | 7/105 [00:05<01:31,  1.08it/s, loss=1.42]\u001b[A\n",
            "Epoch 1/10:   8%|▊         | 8/105 [00:05<00:56,  1.72it/s, loss=1.42]\u001b[A\n",
            "Epoch 1/10:   8%|▊         | 8/105 [00:05<00:56,  1.72it/s, loss=1.5] \u001b[A\n",
            "Epoch 1/10:   9%|▊         | 9/105 [00:05<00:48,  1.98it/s, loss=1.5]\u001b[A\n",
            "Epoch 1/10:   9%|▊         | 9/105 [00:05<00:48,  1.98it/s, loss=2.67]\u001b[A\n",
            "Epoch 1/10:  10%|▉         | 10/105 [00:05<00:43,  2.17it/s, loss=2.67]\u001b[A\n",
            "Epoch 1/10:  10%|█         | 11/105 [00:06<00:43,  2.17it/s, loss=2.09]\u001b[A\n",
            "Epoch 1/10:  11%|█▏        | 12/105 [00:06<00:32,  2.90it/s, loss=2.09]\u001b[A\n",
            "Epoch 1/10:  11%|█▏        | 12/105 [00:06<00:32,  2.90it/s, loss=2.29]\u001b[A\n",
            "Epoch 1/10:  12%|█▏        | 13/105 [00:06<00:31,  2.97it/s, loss=2.29]\u001b[A\n",
            "Epoch 1/10:  12%|█▏        | 13/105 [00:06<00:31,  2.97it/s, loss=2.18]\u001b[A\n",
            "Epoch 1/10:  13%|█▎        | 14/105 [00:06<00:30,  2.96it/s, loss=2.18]\u001b[A\n",
            "Epoch 1/10:  13%|█▎        | 14/105 [00:07<00:30,  2.96it/s, loss=1.87]\u001b[A\n",
            "Epoch 1/10:  14%|█▍        | 15/105 [00:07<00:45,  1.99it/s, loss=1.87]\u001b[A\n",
            "Epoch 1/10:  15%|█▌        | 16/105 [00:07<00:44,  1.99it/s, loss=1.34]\u001b[A\n",
            "Epoch 1/10:  16%|█▌        | 17/105 [00:07<00:30,  2.87it/s, loss=1.34]\u001b[A\n",
            "Epoch 1/10:  16%|█▌        | 17/105 [00:08<00:30,  2.87it/s, loss=1.97]\u001b[A\n",
            "Epoch 1/10:  17%|█▋        | 18/105 [00:08<00:29,  2.97it/s, loss=1.97]\u001b[A\n",
            "Epoch 1/10:  17%|█▋        | 18/105 [00:08<00:29,  2.97it/s, loss=1.36]\u001b[A\n",
            "Epoch 1/10:  18%|█▊        | 19/105 [00:08<00:29,  2.93it/s, loss=1.36]\u001b[A\n",
            "Epoch 1/10:  18%|█▊        | 19/105 [00:08<00:29,  2.93it/s, loss=1.38]\u001b[A\n",
            "Epoch 1/10:  19%|█▉        | 20/105 [00:08<00:27,  3.10it/s, loss=1.38]\u001b[A\n",
            "Epoch 1/10:  19%|█▉        | 20/105 [00:09<00:27,  3.10it/s, loss=1.79]\u001b[A\n",
            "Epoch 1/10:  20%|██        | 21/105 [00:09<00:29,  2.88it/s, loss=1.79]\u001b[A\n",
            "Epoch 1/10:  21%|██        | 22/105 [00:09<00:28,  2.88it/s, loss=1.46]\u001b[A\n",
            "Epoch 1/10:  22%|██▏       | 23/105 [00:09<00:22,  3.72it/s, loss=1.46]\u001b[A\n",
            "Epoch 1/10:  22%|██▏       | 23/105 [00:09<00:22,  3.72it/s, loss=1.83]\u001b[A\n",
            "Epoch 1/10:  23%|██▎       | 24/105 [00:09<00:23,  3.43it/s, loss=1.83]\u001b[A\n",
            "Epoch 1/10:  23%|██▎       | 24/105 [00:10<00:23,  3.43it/s, loss=1.26]\u001b[A\n",
            "Epoch 1/10:  24%|██▍       | 25/105 [00:10<00:22,  3.50it/s, loss=1.26]\u001b[A\n",
            "Epoch 1/10:  24%|██▍       | 25/105 [00:10<00:22,  3.50it/s, loss=3.11]\u001b[A\n",
            "Epoch 1/10:  25%|██▍       | 26/105 [00:10<00:22,  3.44it/s, loss=3.11]\u001b[A\n",
            "Epoch 1/10:  25%|██▍       | 26/105 [00:10<00:22,  3.44it/s, loss=1.47]\u001b[A\n",
            "Epoch 1/10:  26%|██▌       | 27/105 [00:10<00:24,  3.16it/s, loss=1.47]\u001b[A\n",
            "Epoch 1/10:  26%|██▌       | 27/105 [00:11<00:24,  3.16it/s, loss=1.6] \u001b[A\n",
            "Epoch 1/10:  27%|██▋       | 28/105 [00:11<00:25,  3.04it/s, loss=1.6]\u001b[A\n",
            "Epoch 1/10:  28%|██▊       | 29/105 [00:11<00:24,  3.04it/s, loss=1.17]\u001b[A\n",
            "Epoch 1/10:  29%|██▊       | 30/105 [00:11<00:19,  3.80it/s, loss=1.17]\u001b[A\n",
            "Epoch 1/10:  29%|██▊       | 30/105 [00:11<00:19,  3.80it/s, loss=1.44]\u001b[A\n",
            "Epoch 1/10:  30%|██▉       | 31/105 [00:11<00:20,  3.68it/s, loss=1.44]\u001b[A\n",
            "Epoch 1/10:  30%|██▉       | 31/105 [00:12<00:20,  3.68it/s, loss=1.37]\u001b[A\n",
            "Epoch 1/10:  30%|███       | 32/105 [00:12<00:20,  3.50it/s, loss=1.37]\u001b[A\n",
            "Epoch 1/10:  30%|███       | 32/105 [00:12<00:20,  3.50it/s, loss=1.27]\u001b[A\n",
            "Epoch 1/10:  31%|███▏      | 33/105 [00:12<00:21,  3.38it/s, loss=1.27]\u001b[A\n",
            "Epoch 1/10:  31%|███▏      | 33/105 [00:12<00:21,  3.38it/s, loss=1.42]\u001b[A\n",
            "Epoch 1/10:  32%|███▏      | 34/105 [00:12<00:22,  3.14it/s, loss=1.42]\u001b[A\n",
            "Epoch 1/10:  32%|███▏      | 34/105 [00:13<00:22,  3.14it/s, loss=1.73]\u001b[A\n",
            "Epoch 1/10:  33%|███▎      | 35/105 [00:13<00:22,  3.13it/s, loss=1.73]\u001b[A\n",
            "Epoch 1/10:  33%|███▎      | 35/105 [00:16<00:22,  3.13it/s, loss=2.08]\u001b[A\n",
            "Epoch 1/10:  34%|███▍      | 36/105 [00:16<01:22,  1.19s/it, loss=2.08]\u001b[A\n",
            "Epoch 1/10:  35%|███▌      | 37/105 [00:17<01:20,  1.19s/it, loss=1.28]\u001b[A\n",
            "Epoch 1/10:  36%|███▌      | 38/105 [00:17<00:49,  1.35it/s, loss=1.28]\u001b[A\n",
            "Epoch 1/10:  36%|███▌      | 38/105 [00:17<00:49,  1.35it/s, loss=1.66]\u001b[A\n",
            "Epoch 1/10:  37%|███▋      | 39/105 [00:17<00:42,  1.57it/s, loss=1.66]\u001b[A\n",
            "Epoch 1/10:  37%|███▋      | 39/105 [00:17<00:42,  1.57it/s, loss=1.32]\u001b[A\n",
            "Epoch 1/10:  38%|███▊      | 40/105 [00:17<00:36,  1.80it/s, loss=1.32]\u001b[A\n",
            "Epoch 1/10:  38%|███▊      | 40/105 [00:18<00:36,  1.80it/s, loss=1.14]\u001b[A\n",
            "Epoch 1/10:  39%|███▉      | 41/105 [00:18<00:32,  1.99it/s, loss=1.14]\u001b[A\n",
            "Epoch 1/10:  39%|███▉      | 41/105 [00:18<00:32,  1.99it/s, loss=1.19]\u001b[A\n",
            "Epoch 1/10:  40%|████      | 42/105 [00:18<00:28,  2.25it/s, loss=1.19]\u001b[A\n",
            "Epoch 1/10:  40%|████      | 42/105 [00:18<00:28,  2.25it/s, loss=1.2] \u001b[A\n",
            "Epoch 1/10:  41%|████      | 43/105 [00:18<00:24,  2.49it/s, loss=1.2]\u001b[A\n",
            "Epoch 1/10:  41%|████      | 43/105 [00:18<00:24,  2.49it/s, loss=1.38]\u001b[A\n",
            "Epoch 1/10:  42%|████▏     | 44/105 [00:18<00:22,  2.71it/s, loss=1.38]\u001b[A\n",
            "Epoch 1/10:  42%|████▏     | 44/105 [00:19<00:22,  2.71it/s, loss=1.68]\u001b[A\n",
            "Epoch 1/10:  43%|████▎     | 45/105 [00:19<00:21,  2.81it/s, loss=1.68]\u001b[A\n",
            "Epoch 1/10:  44%|████▍     | 46/105 [00:19<00:20,  2.81it/s, loss=1.06]\u001b[A\n",
            "Epoch 1/10:  45%|████▍     | 47/105 [00:19<00:15,  3.69it/s, loss=1.06]\u001b[A\n",
            "Epoch 1/10:  45%|████▍     | 47/105 [00:19<00:15,  3.69it/s, loss=1.19]\u001b[A\n",
            "Epoch 1/10:  46%|████▌     | 48/105 [00:19<00:16,  3.42it/s, loss=1.19]\u001b[A\n",
            "Epoch 1/10:  46%|████▌     | 48/105 [00:20<00:16,  3.42it/s, loss=1.64]\u001b[A\n",
            "Epoch 1/10:  47%|████▋     | 49/105 [00:20<00:19,  2.84it/s, loss=1.64]\u001b[A\n",
            "Epoch 1/10:  47%|████▋     | 49/105 [00:20<00:19,  2.84it/s, loss=1.2] \u001b[A\n",
            "Epoch 1/10:  48%|████▊     | 50/105 [00:20<00:18,  3.00it/s, loss=1.2]\u001b[A\n",
            "Epoch 1/10:  48%|████▊     | 50/105 [00:21<00:18,  3.00it/s, loss=0.943]\u001b[A\n",
            "Epoch 1/10:  49%|████▊     | 51/105 [00:21<00:17,  3.07it/s, loss=0.943]\u001b[A\n",
            "Epoch 1/10:  49%|████▊     | 51/105 [00:21<00:17,  3.07it/s, loss=0.97] \u001b[A\n",
            "Epoch 1/10:  50%|████▉     | 52/105 [00:21<00:17,  3.06it/s, loss=0.97]\u001b[A\n",
            "Epoch 1/10:  50%|████▉     | 52/105 [00:21<00:17,  3.06it/s, loss=1.1] \u001b[A\n",
            "Epoch 1/10:  50%|█████     | 53/105 [00:21<00:16,  3.08it/s, loss=1.1]\u001b[A\n",
            "Epoch 1/10:  50%|█████     | 53/105 [00:22<00:16,  3.08it/s, loss=1.13]\u001b[A\n",
            "Epoch 1/10:  51%|█████▏    | 54/105 [00:22<00:17,  2.98it/s, loss=1.13]\u001b[A\n",
            "Epoch 1/10:  51%|█████▏    | 54/105 [00:22<00:17,  2.98it/s, loss=1.53]\u001b[A\n",
            "Epoch 1/10:  52%|█████▏    | 55/105 [00:22<00:16,  3.09it/s, loss=1.53]\u001b[A\n",
            "Epoch 1/10:  53%|█████▎    | 56/105 [00:22<00:15,  3.09it/s, loss=0.797]\u001b[A\n",
            "Epoch 1/10:  54%|█████▍    | 57/105 [00:22<00:11,  4.14it/s, loss=0.797]\u001b[A\n",
            "Epoch 1/10:  54%|█████▍    | 57/105 [00:23<00:11,  4.14it/s, loss=1.23] \u001b[A\n",
            "Epoch 1/10:  55%|█████▌    | 58/105 [00:23<00:12,  3.63it/s, loss=1.23]\u001b[A\n",
            "Epoch 1/10:  55%|█████▌    | 58/105 [00:23<00:12,  3.63it/s, loss=1.37]\u001b[A\n",
            "Epoch 1/10:  56%|█████▌    | 59/105 [00:23<00:12,  3.60it/s, loss=1.37]\u001b[A\n",
            "Epoch 1/10:  56%|█████▌    | 59/105 [00:23<00:12,  3.60it/s, loss=1.22]\u001b[A\n",
            "Epoch 1/10:  57%|█████▋    | 60/105 [00:23<00:12,  3.49it/s, loss=1.22]\u001b[A\n",
            "Epoch 1/10:  57%|█████▋    | 60/105 [00:24<00:12,  3.49it/s, loss=1.47]\u001b[A\n",
            "Epoch 1/10:  58%|█████▊    | 61/105 [00:24<00:14,  3.10it/s, loss=1.47]\u001b[A\n",
            "Epoch 1/10:  58%|█████▊    | 61/105 [00:24<00:14,  3.10it/s, loss=1.69]\u001b[A\n",
            "Epoch 1/10:  59%|█████▉    | 62/105 [00:24<00:13,  3.08it/s, loss=1.69]\u001b[A\n",
            "Epoch 1/10:  59%|█████▉    | 62/105 [00:24<00:13,  3.08it/s, loss=0.689]\u001b[A\n",
            "Epoch 1/10:  60%|██████    | 63/105 [00:24<00:13,  3.21it/s, loss=0.689]\u001b[A\n",
            "Epoch 1/10:  60%|██████    | 63/105 [00:24<00:13,  3.21it/s, loss=0.866]\u001b[A\n",
            "Epoch 1/10:  61%|██████    | 64/105 [00:24<00:12,  3.21it/s, loss=0.866]\u001b[A\n",
            "Epoch 1/10:  61%|██████    | 64/105 [00:25<00:12,  3.21it/s, loss=1.21] \u001b[A\n",
            "Epoch 1/10:  62%|██████▏   | 65/105 [00:25<00:12,  3.15it/s, loss=1.21]\u001b[A\n",
            "Epoch 1/10:  62%|██████▏   | 65/105 [00:25<00:12,  3.15it/s, loss=1.72]\u001b[A\n",
            "Epoch 1/10:  63%|██████▎   | 66/105 [00:25<00:13,  2.95it/s, loss=1.72]\u001b[A\n",
            "Epoch 1/10:  64%|██████▍   | 67/105 [00:25<00:12,  2.95it/s, loss=0.89]\u001b[A\n",
            "Epoch 1/10:  65%|██████▍   | 68/105 [00:25<00:09,  3.97it/s, loss=0.89]\u001b[A\n",
            "Epoch 1/10:  65%|██████▍   | 68/105 [00:26<00:09,  3.97it/s, loss=1.28]\u001b[A\n",
            "Epoch 1/10:  66%|██████▌   | 69/105 [00:26<00:09,  3.72it/s, loss=1.28]\u001b[A\n",
            "Epoch 1/10:  66%|██████▌   | 69/105 [00:26<00:09,  3.72it/s, loss=0.942]\u001b[A\n",
            "Epoch 1/10:  67%|██████▋   | 70/105 [00:26<00:10,  3.40it/s, loss=0.942]\u001b[A\n",
            "Epoch 1/10:  67%|██████▋   | 70/105 [00:26<00:10,  3.40it/s, loss=0.995]\u001b[A\n",
            "Epoch 1/10:  68%|██████▊   | 71/105 [00:26<00:09,  3.46it/s, loss=0.995]\u001b[A\n",
            "Epoch 1/10:  68%|██████▊   | 71/105 [00:27<00:09,  3.46it/s, loss=1]    \u001b[A\n",
            "Epoch 1/10:  69%|██████▊   | 72/105 [00:27<00:10,  3.17it/s, loss=1]\u001b[A\n",
            "Epoch 1/10:  69%|██████▊   | 72/105 [00:27<00:10,  3.17it/s, loss=0.785]\u001b[A\n",
            "Epoch 1/10:  70%|██████▉   | 73/105 [00:27<00:10,  3.16it/s, loss=0.785]\u001b[A\n",
            "Epoch 1/10:  70%|██████▉   | 73/105 [00:27<00:10,  3.16it/s, loss=0.871]\u001b[A\n",
            "Epoch 1/10:  70%|███████   | 74/105 [00:27<00:09,  3.10it/s, loss=0.871]\u001b[A\n",
            "Epoch 1/10:  70%|███████   | 74/105 [00:28<00:09,  3.10it/s, loss=1.39] \u001b[A\n",
            "Epoch 1/10:  71%|███████▏  | 75/105 [00:28<00:10,  3.00it/s, loss=1.39]\u001b[A\n",
            "Epoch 1/10:  71%|███████▏  | 75/105 [00:28<00:10,  3.00it/s, loss=1.05]\u001b[A\n",
            "Epoch 1/10:  72%|███████▏  | 76/105 [00:28<00:09,  2.90it/s, loss=1.05]\u001b[A\n",
            "Epoch 1/10:  72%|███████▏  | 76/105 [00:29<00:09,  2.90it/s, loss=1.72]\u001b[A\n",
            "Epoch 1/10:  73%|███████▎  | 77/105 [00:29<00:09,  2.87it/s, loss=1.72]\u001b[A\n",
            "Epoch 1/10:  73%|███████▎  | 77/105 [00:29<00:09,  2.87it/s, loss=1.38]\u001b[A\n",
            "Epoch 1/10:  74%|███████▍  | 78/105 [00:29<00:09,  2.97it/s, loss=1.38]\u001b[A\n",
            "Epoch 1/10:  75%|███████▌  | 79/105 [00:29<00:08,  2.97it/s, loss=1.21]\u001b[A\n",
            "Epoch 1/10:  76%|███████▌  | 80/105 [00:29<00:06,  3.76it/s, loss=1.21]\u001b[A\n",
            "Epoch 1/10:  76%|███████▌  | 80/105 [00:30<00:06,  3.76it/s, loss=1.07]\u001b[A\n",
            "Epoch 1/10:  77%|███████▋  | 81/105 [00:30<00:06,  3.52it/s, loss=1.07]\u001b[A\n",
            "Epoch 1/10:  77%|███████▋  | 81/105 [00:30<00:06,  3.52it/s, loss=1.78]\u001b[A\n",
            "Epoch 1/10:  78%|███████▊  | 82/105 [00:30<00:06,  3.30it/s, loss=1.78]\u001b[A\n",
            "Epoch 1/10:  78%|███████▊  | 82/105 [00:30<00:06,  3.30it/s, loss=0.963]\u001b[A\n",
            "Epoch 1/10:  79%|███████▉  | 83/105 [00:30<00:06,  3.20it/s, loss=0.963]\u001b[A\n",
            "Epoch 1/10:  79%|███████▉  | 83/105 [00:31<00:06,  3.20it/s, loss=1.07] \u001b[A\n",
            "Epoch 1/10:  80%|████████  | 84/105 [00:31<00:09,  2.29it/s, loss=1.07]\u001b[A\n",
            "Epoch 1/10:  80%|████████  | 84/105 [00:31<00:09,  2.29it/s, loss=1.28]\u001b[A\n",
            "Epoch 1/10:  81%|████████  | 85/105 [00:31<00:08,  2.39it/s, loss=1.28]\u001b[A\n",
            "Epoch 1/10:  81%|████████  | 85/105 [00:32<00:08,  2.39it/s, loss=1.18]\u001b[A\n",
            "Epoch 1/10:  82%|████████▏ | 86/105 [00:32<00:07,  2.45it/s, loss=1.18]\u001b[A\n",
            "Epoch 1/10:  82%|████████▏ | 86/105 [00:32<00:07,  2.45it/s, loss=1.12]\u001b[A\n",
            "Epoch 1/10:  83%|████████▎ | 87/105 [00:32<00:07,  2.53it/s, loss=1.12]\u001b[A\n",
            "Epoch 1/10:  83%|████████▎ | 87/105 [00:33<00:07,  2.53it/s, loss=1.15]\u001b[A\n",
            "Epoch 1/10:  84%|████████▍ | 88/105 [00:33<00:06,  2.53it/s, loss=1.15]\u001b[A\n",
            "Epoch 1/10:  84%|████████▍ | 88/105 [00:37<00:06,  2.53it/s, loss=1.82]\u001b[A\n",
            "Epoch 1/10:  85%|████████▍ | 89/105 [00:37<00:24,  1.53s/it, loss=1.82]\u001b[A\n",
            "Epoch 1/10:  85%|████████▍ | 89/105 [00:37<00:24,  1.53s/it, loss=0.939]\u001b[A\n",
            "Epoch 1/10:  86%|████████▌ | 90/105 [00:37<00:17,  1.19s/it, loss=0.939]\u001b[A\n",
            "Epoch 1/10:  86%|████████▌ | 90/105 [00:37<00:17,  1.19s/it, loss=0.987]\u001b[A\n",
            "Epoch 1/10:  87%|████████▋ | 91/105 [00:37<00:13,  1.07it/s, loss=0.987]\u001b[A\n",
            "Epoch 1/10:  88%|████████▊ | 92/105 [00:38<00:12,  1.07it/s, loss=1.15] \u001b[A\n",
            "Epoch 1/10:  89%|████████▊ | 93/105 [00:38<00:07,  1.71it/s, loss=1.15]\u001b[A\n",
            "Epoch 1/10:  89%|████████▊ | 93/105 [00:38<00:07,  1.71it/s, loss=1.26]\u001b[A\n",
            "Epoch 1/10:  90%|████████▉ | 94/105 [00:38<00:05,  1.95it/s, loss=1.26]\u001b[A"
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "model.to('cuda')\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "num_epochs = 10\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "    print(f'a')\n",
        "    model.train()\n",
        "    bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "    for images, targets in bar:\n",
        "        # print('b')\n",
        "        images = [image.to('cuda') for image in images]\n",
        "        targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
        "        # import torch.nn.functional as F\n",
        "        # images = F.interpolate(\n",
        "        #     images,\n",
        "        #     size=224,\n",
        "        #     mode='bilinear',\n",
        "        #     align_corners=False\n",
        "        # )\n",
        "\n",
        "        # targets['masks'] = F.interpolate(\n",
        "        #     targets['masks'],\n",
        "        #     size=224,\n",
        "        #     mode='nearest'\n",
        "        # )\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bar.set_postfix(loss=losses.detach().cpu().item())\n",
        "        bar.update()\n",
        "    lr_scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {losses.item()}\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2813e4ba",
      "metadata": {
        "id": "2813e4ba",
        "outputId": "2e8b2f3b-8d24-4649-de8a-3a19ac7b5e57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference:   0%|          | 0/53 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_convert\n\u001b[32m      9\u001b[39m bar = tqdm(train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mInference\u001b[39m\u001b[33m\"\u001b[39m, total=\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:735\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    733\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    738\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    741\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1493\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1490\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1493\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1496\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1455\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1452\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1454\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1455\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1456\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1457\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1286\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1274\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1275\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1283\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1288\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1290\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1291\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/queues.py:111\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    110\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/multiprocessing/connection.py:1148\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/test/lib/python3.13/selectors.py:398\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    396\u001b[39m ready = []\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "model = maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "model.to('cuda')\n",
        "model.eval()\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "bar = tqdm(train_loader, desc=\"Inference\", total=len(train_loader))\n",
        "for images, targets in bar:\n",
        "    images = list(image.to('cuda') for image in images)\n",
        "    targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(images)\n",
        "\n",
        "    for i, prediction in enumerate(predictions):\n",
        "        boxes = prediction['boxes'].cpu().numpy()\n",
        "        masks = prediction['masks'].cpu().numpy()\n",
        "        labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "        # Process the predictions as needed\n",
        "        print(f\"Image {i}:\")\n",
        "        print(\"Boxes:\", boxes)\n",
        "        print(\"Masks:\", masks)\n",
        "        print(\"Labels:\", labels)\n",
        "\n",
        "    bar.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c4e071",
      "metadata": {
        "id": "c7c4e071"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.get_model(\n",
        "        args.model, weights=args.weights, weights_backbone=args.weights_backbone, num_classes=num_classes, **kwargs\n",
        "    )\n",
        "model.roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=len(class_names),bias=True)\n",
        "model.roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=len(class_names)*4,bias=True)\n",
        "model.roi_heads.mask_predictor.mask_fcn_logits = nn.Conv2d(256, len(class_names),kernel_size=(1,1),stride=(1,1))\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a3e318",
      "metadata": {
        "id": "13a3e318"
      },
      "outputs": [],
      "source": [
        "class EnhancedMaskRCNN(MaskRCNN):\n",
        "    def __init__(self, backbone, num_classes=None, **kwargs):\n",
        "        super().__init__(backbone, num_classes, **kwargs)\n",
        "        # 添加邊界感知分支\n",
        "        self.boundary_head = self._build_boundary_head()\n",
        "\n",
        "    def _build_boundary_head(self):\n",
        "        layers = [\n",
        "            torch.nn.Conv2d(256, 256, 3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv2d(256, 256, 3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv2d(256, 1, 1)  # 邊界檢測輸出\n",
        "        ]\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        # 原始輸出\n",
        "        outputs = super().forward(images, targets)\n",
        "\n",
        "        # 邊界檢測分支\n",
        "        if self.training and targets is not None:\n",
        "            boundary_maps = self.boundary_head(outputs['features'])\n",
        "            outputs['boundary_loss'] = self.compute_boundary_loss(boundary_maps, targets)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def create_model(num_classes=5, pretrained=True):\n",
        "    backbone = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "    # backbone = torchvision.models._utils.IntermediateLayerGetter(\n",
        "    #     backbone, return_layers={'layer4': 'out'}\n",
        "    # )\n",
        "    # backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "\n",
        "    model = EnhancedMaskRCNN(\n",
        "        backbone,\n",
        "        num_classes=num_classes,\n",
        "        min_size=512,\n",
        "        max_size=512\n",
        "    )\n",
        "\n",
        "    # 修改分類頭\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # 修改mask頭\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask, 256, num_classes\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df76213",
      "metadata": {
        "id": "9df76213",
        "outputId": "bdb340bb-a025-47a7-dd3f-37fcf35cc3ea"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'ResNet' object has no attribute 'out_channels'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m SGD(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n",
            "Cell \u001b[0;32mIn[46], line 35\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(num_classes, pretrained)\u001b[0m\n\u001b[1;32m     29\u001b[0m backbone \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mresnet50(weights\u001b[38;5;241m=\u001b[39mResNet50_Weights\u001b[38;5;241m.\u001b[39mDEFAULT)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# backbone = torchvision.models._utils.IntermediateLayerGetter(\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     backbone, return_layers={'layer4': 'out'}\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEnhancedMaskRCNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# min_size=512,\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_size=512\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 修改分類頭\u001b[39;00m\n\u001b[1;32m     43\u001b[0m in_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor\u001b[38;5;241m.\u001b[39mcls_score\u001b[38;5;241m.\u001b[39min_features\n",
            "Cell \u001b[0;32mIn[46], line 3\u001b[0m, in \u001b[0;36mEnhancedMaskRCNN.__init__\u001b[0;34m(self, backbone, num_classes, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, backbone, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# 添加邊界感知分支\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboundary_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_boundary_head()\n",
            "File \u001b[0;32m~/anaconda3/envs/vdl-lab3/lib/python3.10/site-packages/torchvision/models/detection/mask_rcnn.py:215\u001b[0m, in \u001b[0;36mMaskRCNN.__init__\u001b[0;34m(self, backbone, num_classes, min_size, max_size, image_mean, image_std, rpn_anchor_generator, rpn_head, rpn_pre_nms_top_n_train, rpn_pre_nms_top_n_test, rpn_post_nms_top_n_train, rpn_post_nms_top_n_test, rpn_nms_thresh, rpn_fg_iou_thresh, rpn_bg_iou_thresh, rpn_batch_size_per_image, rpn_positive_fraction, rpn_score_thresh, box_roi_pool, box_head, box_predictor, box_score_thresh, box_nms_thresh, box_detections_per_img, box_fg_iou_thresh, box_bg_iou_thresh, box_batch_size_per_image, box_positive_fraction, bbox_reg_weights, mask_roi_pool, mask_head, mask_predictor, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes should be None when mask_predictor is specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m out_channels \u001b[38;5;241m=\u001b[39m \u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask_roi_pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     mask_roi_pool \u001b[38;5;241m=\u001b[39m MultiScaleRoIAlign(featmap_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m], output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, sampling_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/vdl-lab3/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'out_channels'"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = create_model().to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "optimizer = SGD(params, lr=0.003, momentum=0.9, weight_decay=0.0005)\n",
        "lr_sched = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# 混合損失函數\n",
        "def hybrid_loss(pred_masks, gt_masks, boundaries):\n",
        "    mask_loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_masks, gt_masks)\n",
        "    boundary_loss = torch.nn.functional.mse_loss(pred_masks, boundaries)\n",
        "    return mask_loss + 0.3 * boundary_loss\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{'masks': t.to(device)} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    lr_sched.step()\n",
        "    print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe24224b",
      "metadata": {
        "id": "fe24224b"
      },
      "outputs": [],
      "source": [
        "def masks_to_coco(results, image_ids):\n",
        "    coco_results = []\n",
        "    for img_id, output in zip(image_ids, results):\n",
        "        for score, mask, label in zip(output['scores'], output['masks'], output['labels']):\n",
        "            rle = binary_mask_to_rle(mask)\n",
        "            coco_results.append({\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": label.item(),\n",
        "                \"segmentation\": rle,\n",
        "                \"score\": score.item()\n",
        "            })\n",
        "    return coco_results\n",
        "\n",
        "def binary_mask_to_rle(mask):\n",
        "    # RLE編碼實現\n",
        "    pixels = mask.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return {'size': list(mask.shape[-2:]), 'counts': runs.tolist()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06683b20",
      "metadata": {
        "id": "06683b20"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_loader = DataLoader(test_set, batch_size=2, shuffle=False)\n",
        "\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        outputs = model(batch.to(device))\n",
        "        results.extend(outputs)\n",
        "\n",
        "# 生成最終提交文件\n",
        "with open('test-results.json', 'w') as f:\n",
        "    json.dump(masks_to_coco(results, test_set.image_ids), f)\n",
        "\n",
        "print(\"Submission file generated!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}