{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8afd8226",
      "metadata": {
        "id": "8afd8226"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tifffile import imread\n",
        "import cv2\n",
        "import skimage.io as sio\n",
        "\n",
        "import albumentations as A\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection import MaskRCNN, FasterRCNN_ResNet50_FPN_Weights, MaskRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from torchvision.ops import box_convert\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "\n",
        "import pathlib\n",
        "import json\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as coco_mask\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5tzWQPzcCgKR",
      "metadata": {
        "id": "5tzWQPzcCgKR"
      },
      "outputs": [],
      "source": [
        "# !gdown https://drive.google.com/file/d/1B0qWNzQZQmfQP7x7o4FDdgb9GvPDoFzI/view --fuzzy\n",
        "# !mkdir ../dataset\n",
        "# !tar -xzf hw3-data-release.tar.gz\n",
        "# !mv test_release/ ../dataset\n",
        "# !mv train/ ../dataset/\n",
        "# !mv test_image_name_to_ids.json ../dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2ce4b413",
      "metadata": {
        "id": "2ce4b413"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import skimage.io as sio\n",
        "from pycocotools import mask as mask_utils\n",
        "\n",
        "\n",
        "def decode_maskobj(mask_obj):\n",
        "    return mask_utils.decode(mask_obj)\n",
        "\n",
        "\n",
        "def encode_mask(binary_mask):\n",
        "    arr = np.asfortranarray(binary_mask).astype(np.uint8)\n",
        "    rle = mask_utils.encode(arr)\n",
        "    rle['counts'] = rle['counts'].decode('utf-8')\n",
        "    return rle\n",
        "\n",
        "\n",
        "def read_maskfile(filepath):\n",
        "    mask_array = sio.imread(filepath)\n",
        "    return mask_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ghBs_VHH32_",
      "metadata": {
        "id": "6ghBs_VHH32_"
      },
      "outputs": [],
      "source": [
        "all_img_size = [224, 224]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8479fbf0",
      "metadata": {
        "id": "8479fbf0"
      },
      "outputs": [],
      "source": [
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, data_type='Train'):\n",
        "        self.root = root_dir\n",
        "        self.transform = transform\n",
        "        self.data_type = data_type\n",
        "        if self.data_type not in ['Train', 'Valid', 'Test']:\n",
        "            raise ValueError('Data type should be in [Train, Valid, Test]')\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "        self.train_coco_path = os.path.join(pathlib.Path(root_dir).parent, 'train_coco.json')\n",
        "        self.val_coco_path = os.path.join(pathlib.Path(root_dir).parent, 'val_coco.json')\n",
        "        self.test_json_path = os.path.join(pathlib.Path(root_dir).parent, 'test_image_name_to_ids.json')\n",
        "\n",
        "        if not os.path.exists(self.train_coco_path) or not os.path.exists(self.val_coco_path):\n",
        "            # self.generate_coco_split_new(self.train_coco_path, self.val_coco_path, split_ratio=0.8)\n",
        "            self.generate_coco_split(self.train_coco_path, self.val_coco_path, split_ratio=0.8)\n",
        "        self.train_coco = COCO(self.train_coco_path)\n",
        "        self.val_coco = COCO(self.val_coco_path)\n",
        "\n",
        "        self.num_classes = len(self.train_coco.loadCats(self.train_coco.getCatIds()))\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for img_dir in os.listdir(self.root):\n",
        "            tmp_dir = os.path.join(self.root, img_dir)\n",
        "\n",
        "            if self.data_type == 'Train' or self.data_type == 'Valid':\n",
        "                img_path = os.path.join(tmp_dir, 'image.tif')\n",
        "\n",
        "                mask_paths = [\n",
        "                    entry.name for entry in pathlib.Path(tmp_dir).iterdir()\n",
        "                    if entry.name.startswith(\"class\") and entry.is_file()\n",
        "                ]\n",
        "\n",
        "                samples.append({'image': img_path, 'masks': mask_paths})\n",
        "            elif self.data_type == 'Test':\n",
        "                test_img_json_path = os.path.join(pathlib.Path(self.root).parent, 'test_image_name_to_ids.json')\n",
        "                with open(test_img_json_path, 'r') as f:\n",
        "                    samples = json.load(f)\n",
        "\n",
        "            else:\n",
        "                raise ValueError('Wrong data type')\n",
        "\n",
        "                # for idx in range(len(samples)):\n",
        "                #     samples[idx]['file_name'] = os.path.join(self.root, samples[idx]['file_name'])\n",
        "        return samples\n",
        "\n",
        "    def mask_to_polygons(self, mask, epsilon=1.0):\n",
        "        contours,_ = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "        polygons = []\n",
        "        for contour in contours:\n",
        "            if len(contour) > 2:\n",
        "                poly = contour.reshape(-1).tolist()\n",
        "                if len(poly) > 4: #Ensures valid polygon\n",
        "                    polygons.append(poly)\n",
        "        return polygons\n",
        "\n",
        "\n",
        "    # 全都改transform後的結果\n",
        "    def generate_coco_split_new(self, train_coco_path, val_coco_path, split_ratio=0.8):\n",
        "        train_data = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
        "        val_data = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
        "        all_labels = []\n",
        "        ann_id = 0\n",
        "        train_ann = 0\n",
        "        val_ann = 0\n",
        "\n",
        "        # 隨機分離樣本索引\n",
        "        indices = list(range(len(self.samples)))\n",
        "        import random\n",
        "        seed = 123\n",
        "        random.Random(seed).shuffle(indices)\n",
        "        split_point = int(len(indices) * split_ratio)\n",
        "        train_indices = indices[:split_point]\n",
        "        val_indices = indices[split_point:]\n",
        "\n",
        "        for dataset_type, indices in [(\"train\", train_indices), (\"val\", val_indices)]:\n",
        "            target_data = train_data if dataset_type == \"train\" else val_data\n",
        "\n",
        "            for idx in indices:\n",
        "                sample = self.samples[idx]\n",
        "                img_path, mask_paths = sample['image'], sample['masks']\n",
        "                img = cv2.imread(img_path)\n",
        "                masks = [cv2.imread(os.path.join(pathlib.Path(img_path).parent, mask_path), cv2.IMREAD_UNCHANGED) for mask_path in mask_paths]\n",
        "\n",
        "                image_entry = {\n",
        "                    \"id\": idx,\n",
        "                    \"file_name\": img_path,\n",
        "                    \"height\": all_img_size[0],\n",
        "                    \"width\": all_img_size[1]\n",
        "                }\n",
        "                target_data[\"images\"].append(image_entry)\n",
        "\n",
        "                for mask in masks:\n",
        "                    unique_values = np.unique(mask)\n",
        "                    # all_labels.append(unique_values)\n",
        "                    for value in unique_values:\n",
        "                        if value == 0:  # Ignore background\n",
        "                            continue\n",
        "\n",
        "                        mask_ = cv2.resize(\n",
        "                            mask,\n",
        "                            (all_img_size[1], all_img_size[0]),\n",
        "                            interpolation=cv2.INTER_NEAREST_EXACT  # 精確最近鄰算法\n",
        "                        )\n",
        "\n",
        "                        object_mask = (mask_ == value).astype(np.uint8) * 255\n",
        "                        polygons = self.mask_to_polygons(object_mask)\n",
        "\n",
        "                        for poly in polygons:\n",
        "                            # ann_id += 1\n",
        "                            if dataset_type == 'train':\n",
        "                                train_ann += 1\n",
        "                                ann_id = train_ann\n",
        "                            else:\n",
        "                                val_ann += 1\n",
        "                                ann_id = val_ann\n",
        "\n",
        "                            target_data[\"annotations\"].append({\n",
        "                                \"id\": ann_id,\n",
        "                                \"image_id\": idx,\n",
        "                                # \"category_id\": int(value),  # Only one category: Nuclei\n",
        "                                \"category_id\": 1,  # Only one category: Nuclei\n",
        "                                \"segmentation\": [poly],\n",
        "                                \"area\": cv2.contourArea(np.array(poly).reshape(-1, 2)),\n",
        "                                \"bbox\": list(cv2.boundingRect(np.array(poly).reshape(-1, 2))),\n",
        "                                \"iscrowd\": 1\n",
        "                            })\n",
        "\n",
        "        # all_labels = np.unique(np.concatenate(all_labels).tolist())\n",
        "\n",
        "        categories = [{\"id\": 1, \"name\": \"Nuclei\"}]\n",
        "        for idx, label in enumerate(all_labels):\n",
        "            categories.append({\"id\": idx+1, \"name\": int(label)})\n",
        "        # categories = []\n",
        "        # for i in range(0, 4):\n",
        "        #     categories.append({\"id\": i, \"name\": i})\n",
        "        train_data[\"categories\"] = categories\n",
        "        val_data[\"categories\"] = categories\n",
        "\n",
        "\n",
        "        # coco_input = {\n",
        "        #     \"images\": images,\n",
        "        #     \"annotations\": annotations,\n",
        "        #     \"categories\": categories\n",
        "        # }\n",
        "\n",
        "        print(f'Saving coco json')\n",
        "\n",
        "        with open(train_coco_path, 'w') as f:\n",
        "            json.dump(train_data, f)\n",
        "        with open(val_coco_path, 'w') as f:\n",
        "            json.dump(val_data, f)\n",
        "\n",
        "    def generate_coco_split(self, train_coco_path, val_coco_path, split_ratio=0.8):\n",
        "        train_data = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
        "        val_data = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
        "        all_labels = []\n",
        "        ann_id = 0\n",
        "        train_ann = 0\n",
        "        val_ann = 0\n",
        "\n",
        "        # 隨機分離樣本索引\n",
        "        indices = list(range(len(self.samples)))\n",
        "        import random\n",
        "        seed = 123\n",
        "        random.Random(seed).shuffle(indices)\n",
        "        split_point = int(len(indices) * split_ratio)\n",
        "        train_indices = indices[:split_point]\n",
        "        val_indices = indices[split_point:]\n",
        "\n",
        "        for dataset_type, indices in [(\"train\", train_indices), (\"val\", val_indices)]:\n",
        "            target_data = train_data if dataset_type == \"train\" else val_data\n",
        "\n",
        "            for idx in indices:\n",
        "                sample = self.samples[idx]\n",
        "                img_path, mask_paths = sample['image'], sample['masks']\n",
        "                img = cv2.imread(img_path) # [H, W, C]\n",
        "                masks = [cv2.imread(os.path.join(pathlib.Path(img_path).parent, mask_path), cv2.IMREAD_UNCHANGED) for mask_path in mask_paths]\n",
        "\n",
        "                image_entry = {\n",
        "                    \"id\": idx,\n",
        "                    \"file_name\": img_path,\n",
        "                    \"height\": img.shape[0],\n",
        "                    \"width\": img.shape[1]\n",
        "                }\n",
        "                target_data[\"images\"].append(image_entry)\n",
        "\n",
        "                for mask_path in mask_paths:\n",
        "                    import re\n",
        "                    m = re.search(r\"class(\\d+)\\.tif\", mask_path)\n",
        "                    class_ = int(m.group(1))\n",
        "                # for mask in masks:\n",
        "                    mask = sio.imread(os.path.join(pathlib.Path(img_path).parent, mask_path))\n",
        "                    unique_values = np.unique(mask)\n",
        "                    # all_labels.append(unique_values)\n",
        "                    for value in unique_values:\n",
        "                        if value == 0:  # Ignore background\n",
        "                            continue\n",
        "\n",
        "                        object_mask = (mask == value).astype(np.uint8)\n",
        "                        # object_mask = (img == value).astype(np.uint8) * 255#.copy()\n",
        "                        # ys, xs = np.where(object_mask)\n",
        "                        # x0, y0, x1, y1 = xs.min(), ys.min(), xs.max(), ys.max()\n",
        "\n",
        "                        rle = encode_mask(object_mask)\n",
        "                        # rle = mask_utils.encode(np.asfortranarray(object_mask))\n",
        "                        # print(rle)\n",
        "                        polygons = self.mask_to_polygons(object_mask)\n",
        "                        # print(rle)\n",
        "                        # for i in rle:\n",
        "                        #     if i != rle[0]:\n",
        "                        #         raise ValueError('not all rle the same')\n",
        "                        # rle = rle[0]\n",
        "                        # rle['counts'] = rle['counts'].decode('utf-8')\n",
        "\n",
        "                        for poly in polygons:\n",
        "                            # print(f'len of poly: {len(polygons)}')\n",
        "                            # ann_id += 1\n",
        "\n",
        "                        # contours, _ = cv2.findContours(object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                            bbox = list(cv2.boundingRect(np.array(poly).reshape(-1, 2)))\n",
        "                            # TEST XYXY BBOX\n",
        "                            # bbox = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]\n",
        "\n",
        "                            # TEST XYWH BBOX\n",
        "                            bbox = bbox\n",
        "\n",
        "\n",
        "                            # print(bbox)\n",
        "                            # for cnt in contours:\n",
        "                            #     x, y, w, h = cv2.boundingRect(cnt)\n",
        "                            #     x1, y1, x2, y2 = x, y, x+w, y+h\n",
        "                            #     bbox.append([x1, y1, x2, y2])\n",
        "                            # if dataset_type == 'train':\n",
        "                            #     train_ann += 1\n",
        "                            #     ann_id = train_ann\n",
        "                            # else:\n",
        "                            #     val_ann += 1\n",
        "                            #     ann_id = val_ann\n",
        "\n",
        "                            target_data[\"annotations\"].append({\n",
        "                                # \"id\": ann_id,\n",
        "                                \"id\": len(target_data[\"annotations\"])+1,\n",
        "                                \"image_id\": idx,\n",
        "                                # \"category_id\": int(value),  # Only one category: Nuclei\n",
        "                                \"category_id\": class_,  # Only one category: Nuclei\n",
        "                                # \"segmentation\": [poly],\n",
        "                                \"segmentation\": rle,\n",
        "                                \"area\": cv2.contourArea(np.array(poly).reshape(-1, 2)),\n",
        "                                # \"bbox\": list(cv2.boundingRect(np.array(poly).reshape(-1, 2))),\n",
        "                                \"bbox\": bbox,   # xyxy\n",
        "                                \"iscrowd\": 1\n",
        "                            })\n",
        "\n",
        "        # all_labels = np.unique(np.concatenate(all_labels).tolist())\n",
        "\n",
        "        # categories = [{\"id\": 1, \"name\": \"Nuclei\"}]\n",
        "        # # for idx, label in enumerate(all_labels):\n",
        "        # #     categories.append({\"id\": idx+1, \"name\": int(label)})\n",
        "        categories = []\n",
        "        for i in range(1, 5):\n",
        "            categories.append({\"id\": i, \"name\": i})\n",
        "        train_data[\"categories\"] = categories\n",
        "        val_data[\"categories\"] = categories\n",
        "\n",
        "\n",
        "        # coco_input = {\n",
        "        #     \"images\": images,\n",
        "        #     \"annotations\": annotations,\n",
        "        #     \"categories\": categories\n",
        "        # }\n",
        "\n",
        "        print(f'Saving coco json')\n",
        "\n",
        "        with open(train_coco_path, 'w') as f:\n",
        "            json.dump(train_data, f)\n",
        "        with open(val_coco_path, 'w') as f:\n",
        "            json.dump(val_data, f)\n",
        "\n",
        "\n",
        "    def poly2mask(self, segmentation, img_size):\n",
        "        \"\"\"\n",
        "        多邊形標註轉二值掩碼\n",
        "        :param segmentation: COCO格式的多邊形坐標列表 [[x1,y1,x2,y2,...]]\n",
        "        :param img_size: 目標圖像尺寸 (height, width)\n",
        "        \"\"\"\n",
        "        # 自動檢測標註類型\n",
        "        if isinstance(segmentation, dict):\n",
        "            # 處理RLE格式\n",
        "            return coco_mask.decode(segmentation)\n",
        "        else:\n",
        "            # 處理多邊形格式\n",
        "            rle = coco_mask.frPyObjects(segmentation, img_size[0], img_size[1])\n",
        "            return coco_mask.decode(rle)\n",
        "\n",
        "    # New get item\n",
        "    def __getitem__(self, index):\n",
        "        if self.data_type == 'Train' or self.data_type == 'Valid':\n",
        "            coco_file = self.train_coco if self.data_type == 'Train' else self.val_coco\n",
        "            img_id = coco_file.dataset['images'][index]['id']\n",
        "            img_ids = coco_file.getImgIds(imgIds=img_id)\n",
        "            img_info = coco_file.loadImgs(img_ids)\n",
        "            image = cv2.imread(img_info[0]['file_name'])# / 255.0\n",
        "            # print(img_info)\n",
        "            # image = Image.open(img_info[0]['file_name']).convert(\"RGB\")\n",
        "            image = self.transform(image) if self.transform is not None else image\n",
        "            # img_size = [img_info[0]['height'], img_info[0]['width']]\n",
        "            img_size = [image.shape[1], image.shape[2]]\n",
        "\n",
        "\n",
        "            boxes = []\n",
        "            masks = []\n",
        "            labels = []\n",
        "            ann_ids = coco_file.getAnnIds(imgIds=img_ids)\n",
        "            annotations = coco_file.loadAnns(ann_ids)\n",
        "            for ann in annotations:\n",
        "                boxes.append(ann['bbox'])\n",
        "                tmp_mask = self.poly2mask(ann['segmentation'], img_size).squeeze()\n",
        "\n",
        "                # mask_ = cv2.resize(\n",
        "                #     tmp_mask,\n",
        "                #     (all_img_size[1], all_img_size[0]),\n",
        "                #     interpolation=cv2.INTER_NEAREST_EXACT  # 精確最近鄰算法\n",
        "                # )\n",
        "                # masks.append(mask_)\n",
        "                masks.append(tmp_mask)\n",
        "                labels.append(ann[\"category_id\"])\n",
        "\n",
        "            # boxes = self.resize_box(boxes, img_size, target_size=all_img_size)\n",
        "            boxes = box_convert(torch.tensor(boxes, dtype=torch.float32), in_fmt='xywh', out_fmt='xyxy')\n",
        "            # masks = torch.as_tensor(np.array(masks), dtype=torch.bool)\n",
        "\n",
        "            target = {'boxes': boxes,\n",
        "                      'masks': torch.as_tensor(np.array(masks), dtype=torch.bool),\n",
        "                      'labels': torch.as_tensor(np.array(labels), dtype=torch.int64)}\n",
        "\n",
        "            return img_id, image, target # box -> xyxy\n",
        "        else:\n",
        "            test_file=[]\n",
        "            with open(self.test_json_path, 'r') as f:\n",
        "                test_file = json.load(f)\n",
        "\n",
        "            image_id = test_file[index]['id']\n",
        "\n",
        "            image_path = os.path.join(self.root, test_file[index]['file_name'])\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image = self.transform(image)\n",
        "\n",
        "            return image_id, image\n",
        "            raise ValueError('This is test, not yet implement')\n",
        "\n",
        "    def resize_box(self, boxes, orig_size, target_size):\n",
        "        # Eat xywh\n",
        "        scale_w = target_size[1] / orig_size[1]\n",
        "        scale_h = target_size[0] / orig_size[0]\n",
        "\n",
        "        # print(f'scale_w = {target_size[1]} / {orig_size[1]}')\n",
        "        # print(f'scale_h = {target_size[0]} / {orig_size[0]}')\n",
        "\n",
        "        # print(f'boxes: {boxes}')\n",
        "        for box in boxes:\n",
        "            box[0] *= scale_w  # x\n",
        "            box[1] *= scale_h  # y\n",
        "            box[2] *= scale_w  # w\n",
        "            box[3] *= scale_h  # h\n",
        "        # boxes[:,0] *= scale_w  # x\n",
        "        # boxes[:,1] *= scale_h  # y\n",
        "        # boxes[:,2] *= scale_w  # w\n",
        "        # boxes[:,3] *= scale_h  # h\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    def __len__(self):\n",
        "        coco_file = self.train_coco if self.data_type == 'Train' else self.val_coco\n",
        "        return len(coco_file.dataset['images'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d0d4d02e",
      "metadata": {
        "id": "d0d4d02e"
      },
      "outputs": [],
      "source": [
        "project_root = '..'\n",
        "train_dir = os.path.join(project_root, 'dataset/train')\n",
        "val_dir = train_dir\n",
        "test_dir = os.path.join(project_root, 'dataset/test_release')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "79f1bf7e",
      "metadata": {
        "id": "79f1bf7e"
      },
      "outputs": [],
      "source": [
        "# train_transform=T.Compose([\n",
        "#     T.ToTensor(),\n",
        "#     T.Resize(size=all_img_size, antialias=True),\n",
        "#     # T.CenterCrop(size=224),\n",
        "#     # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "# train_set = MedicalDataset(root_dir=train_dir, transform=train_transform, data_type='Train')\n",
        "\n",
        "# print(train_set[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0a354976",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a354976",
        "outputId": "ab047ee3-64d1-443f-86b2-5b84c27d2c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.24s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.20s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.09s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.22s)\n",
            "creating index...\n",
            "index created!\n",
            "id: 78\n",
            "img: torch.Size([3, 224, 224])\n",
            "mask: torch.Size([397, 1302, 1730])\n",
            "box: (tensor([[ 124.,  489.,  154.,  519.],\n",
            "        [1656.,  830., 1687.,  864.],\n",
            "        [ 252., 1039.,  324., 1135.],\n",
            "        ...,\n",
            "        [ 649., 1204.,  685., 1257.],\n",
            "        [1020.,  512., 1052.,  561.],\n",
            "        [1065.,  504., 1087.,  556.]]), torch.Size([397, 4]))\n"
          ]
        }
      ],
      "source": [
        "# train_transform = A.Compose([\n",
        "#     A.HorizontalFlip(p=0.5),\n",
        "#     A.VerticalFlip(p=0.3),\n",
        "#     A.Rotate(limit=15, p=0.4),\n",
        "#     A.CLAHE(p=0.5),\n",
        "#     A.GridDistortion(p=0.2),\n",
        "#     A.RandomBrightnessContrast(p=0.3)\n",
        "# ], additional_targets={'mask': 'mask'})\n",
        "train_transform=T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize(size=all_img_size, antialias=True),\n",
        "    # T.CenterCrop(size=224),\n",
        "    # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = train_transform\n",
        "\n",
        "train_set = MedicalDataset(root_dir=train_dir, transform=train_transform, data_type='Train')\n",
        "val_set = MedicalDataset(root_dir=val_dir, transform=val_transform, data_type='Valid')\n",
        "sample = train_set[163]\n",
        "id = sample[0]\n",
        "img = sample[1]\n",
        "masks = sample[2]['masks']\n",
        "boxes = sample[2]['boxes']\n",
        "print(f'id: {id}')\n",
        "print(f'img: {img.shape}')\n",
        "print(f'mask: {masks.shape}')\n",
        "print(f'box: {boxes, boxes.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "90d8a307",
      "metadata": {
        "id": "90d8a307"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "def show_image_with_boxes_and_masks(img, boxes, box_mode,  masks, title=''):\n",
        "    \"\"\"\n",
        "    img: Tensor [3, H, W]\n",
        "    boxes: Tensor [N, 4]\n",
        "    masks: Tensor [N, H, W]\n",
        "    \"\"\"\n",
        "    # 轉成 NumPy 格式並調整 shape 為 [H, W, 3]\n",
        "    img_np = img.cpu().numpy().transpose(1, 2, 0)  # [H, W, 3]\n",
        "    img_np = np.clip(img_np, 0, 1)  # 正規化範圍\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img_np)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    num_objects = boxes.shape[0]\n",
        "    for i in range(num_objects):\n",
        "        # 畫邊界框\n",
        "        if box_mode == 'xyxy':\n",
        "            x1, y1, x2, y2 = boxes[i].cpu().numpy()\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "        elif box_mode == 'xywh':\n",
        "            x1, y1, w, h = boxes[i].cpu().numpy()\n",
        "        else:\n",
        "            raise ValueError('show image wrong box mode')\n",
        "        rect = patches.Rectangle((x1, y1), w, h, linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # 畫遮罩\n",
        "        mask = masks[i].cpu().numpy()\n",
        "        ax.imshow(np.ma.masked_where(mask == 0, mask), cmap='cool', alpha=0.5)\n",
        "\n",
        "    # plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8a7d722c",
      "metadata": {
        "id": "8a7d722c"
      },
      "outputs": [],
      "source": [
        "def json_check_maskonimg(cc, idx, mode=None, box_mode=None):\n",
        "    if mode == 'val':\n",
        "        # cc = COCO('/home/bhg/visual_dl/lab3/dataset/val_coco.json')\n",
        "        imgAnn=cc.dataset['annotations'][idx]\n",
        "        imgid=imgAnn['image_id']\n",
        "        img_info=cc.loadImgs(imgid)[0]\n",
        "        # print(img_info)\n",
        "    elif mode == 'test':\n",
        "        imgid=cc[idx]['image_id']\n",
        "    elif mode == 'val_on_eval':\n",
        "        coco=COCO('/home/bhg/visual_dl/lab3/dataset/val_coco.json')\n",
        "        target=cc[idx]\n",
        "        print(target)\n",
        "        imgid=target['image_id']\n",
        "        img_info=coco.loadImgs(imgid)[0]\n",
        "        imgAnn=target\n",
        "        print(f'ann: {imgAnn}')\n",
        "    else:\n",
        "        raise ValueError('json check mask on img wrong mode')\n",
        "    imgpath=img_info['file_name']\n",
        "    # print(imgpath)\n",
        "    img=cv2.imread(imgpath)\n",
        "    # print(img.shape)\n",
        "    imgtensor=T.ToTensor()(img)\n",
        "    # print(imgtensor.shape)\n",
        "    box=imgAnn['bbox']\n",
        "    # print(f'before box: {box}')\n",
        "    boxtensor=torch.tensor([imgAnn['bbox']])\n",
        "    print(f'box: {boxtensor}')\n",
        "    seg=imgAnn['segmentation']\n",
        "    mask=torch.tensor([decode_maskobj(seg)])\n",
        "    # print(mask.shape)\n",
        "\n",
        "    show_image_with_boxes_and_masks(imgtensor, boxtensor, box_mode, mask, 'rr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2fa9f767",
      "metadata": {
        "id": "2fa9f767"
      },
      "outputs": [],
      "source": [
        "# cc=COCO('../dataset/val_coco.json')\n",
        "# json_check_maskonimg(cc, 0, 'val', 'xywh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "35918d5e",
      "metadata": {
        "id": "35918d5e"
      },
      "outputs": [],
      "source": [
        "# with open('../results/1_res.json', 'r') as f:\n",
        "#     tt = json.load(f)\n",
        "# json_check_maskonimg(tt, 606, 'val_on_eval', 'xywh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "93m5-5g8IUGS",
      "metadata": {
        "id": "93m5-5g8IUGS"
      },
      "outputs": [],
      "source": [
        "max_choices = 60\n",
        "\n",
        "def custom_collate(batch):\n",
        "    img_ids = []\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    # print(batch[0][1])\n",
        "\n",
        "    for img_id, img, target in batch:\n",
        "        img_ids.append(img_id)\n",
        "        images.append(img)\n",
        "        # print(type(target['boxes']))\n",
        "        keep_idx = torch.randperm(target['boxes'].shape[0])[:max_choices]\n",
        "        # n = target['boxes'].shape[0]\n",
        "        targets.append({\n",
        "            'boxes': target['boxes'][keep_idx],\n",
        "            'labels': target['labels'][keep_idx],\n",
        "            'masks': target['masks'][keep_idx]\n",
        "        })\n",
        "\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return img_ids, images, targets\n",
        "\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "NUM_WORKER = 8\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER, pin_memory=False, collate_fn=custom_collate)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKER, pin_memory=False, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9DWHlzYRFNZW",
      "metadata": {
        "id": "9DWHlzYRFNZW"
      },
      "outputs": [],
      "source": [
        "def predictions_to_coco(image_ids, predictions, is_test=False):\n",
        "    coco_results = []\n",
        "    for img_id, prediction in zip(image_ids, predictions):\n",
        "        if is_test:\n",
        "            test_file = []\n",
        "\n",
        "            with open(f'../dataset/test_image_name_to_ids.json', 'r') as f:\n",
        "                test_file = json.load(f)\n",
        "\n",
        "            # print(f'img_id: {img_id}')\n",
        "            img_info = test_file[img_id-1]\n",
        "            # print(info)\n",
        "            orig_h, orig_w = img_info['height'], img_info['width']\n",
        "        else:\n",
        "            val_file = COCO(f'../dataset/val_coco.json')\n",
        "\n",
        "            img_info = val_file.loadImgs(img_id)\n",
        "            # print(img_info)\n",
        "            orig_h, orig_w = img_info[0]['height'], img_info[0]['width']\n",
        "        for score, mask, label, box in zip(prediction['scores'], prediction['masks'], prediction['labels'], prediction['boxes']):\n",
        "            mask = cv2.resize(\n",
        "                    mask.squeeze().detach().cpu().numpy(),\n",
        "                    (orig_w, orig_h),\n",
        "                    interpolation=cv2.INTER_NEAREST_EXACT  # 精確最近鄰算法\n",
        "                )\n",
        "\n",
        "            rle = encode_mask(mask)\n",
        "\n",
        "            box_convert(box, in_fmt='xyxy', out_fmt='xywh')\n",
        "\n",
        "            scale_h = orig_h / all_img_size[0]\n",
        "            scale_w = orig_w / all_img_size[1]\n",
        "            box[0] *= scale_w\n",
        "            box[1] *= scale_h\n",
        "            box[2] *= scale_w\n",
        "            box[3] *= scale_h\n",
        "\n",
        "            coco_results.append({\n",
        "                \"image_id\": int(img_id),\n",
        "                \"bbox\": box.tolist(),\n",
        "                \"score\": score.item(),\n",
        "                \"category_id\": label.item(),\n",
        "                \"segmentation\": rle,\n",
        "            })\n",
        "\n",
        "    return coco_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1f27f892",
      "metadata": {
        "id": "1f27f892"
      },
      "outputs": [],
      "source": [
        "def do_coco_eval(dt_path, gt_path):\n",
        "    gt = COCO(gt_path)\n",
        "    dt = gt.loadRes(dt_path)\n",
        "    coco_eval = COCOeval(gt, dt, 'segm')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    return coco_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0fed2186",
      "metadata": {
        "id": "0fed2186"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader, val_coco, epoch, device):\n",
        "    # 初始化 COCO 格式儲存器\n",
        "    coco_gt = val_coco  # 需提前加載驗證集註解文件\n",
        "    coco_results = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        bar = tqdm(val_loader, desc='Eval', leave=False)\n",
        "        for img_ids, images, targets in bar:\n",
        "            images = [img.to(device) for img in images]\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            coco_results += predictions_to_coco(img_ids, outputs, False)\n",
        "            bar.update()\n",
        "\n",
        "        # bar.close()\n",
        "\n",
        "    save_coco_results_path = f'../results/{epoch}_res.json'\n",
        "    print(coco_results)\n",
        "    # model_output_to_json(coco_results)\n",
        "    with open(save_coco_results_path, 'w') as f:\n",
        "        json.dump(coco_results, f)\n",
        "\n",
        "\n",
        "    # # 評估計算\n",
        "    # coco_dt = coco_gt.loadRes(coco_results)\n",
        "    # coco_eval = COCOeval(coco_gt, coco_dt, 'segm')\n",
        "    # coco_eval.evaluate()\n",
        "    # coco_eval.accumulate()\n",
        "    # coco_eval.summarize()\n",
        "\n",
        "    # with open(f'../results/{epoch}_coco_stats.json', 'w') as f:\n",
        "    #   json.dump(coco_eval.stats.tolist(), f)\n",
        "\n",
        "    # return coco_eval.stats  # 返回 AP 系列指標"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "86b46772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86b46772",
        "outputId": "0b8ef6d8-c499-474e-d7a7-2da1a176420a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# num_classes=train_set.num_classes\n",
        "num_classes = 4\n",
        "print(num_classes)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "04b7d57f",
      "metadata": {
        "id": "04b7d57f"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes):\n",
        "    from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "    from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "\n",
        "    # 2. 替換分類器\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)  # 自定義類別數\n",
        "\n",
        "    # 3. 替換掩碼分類器\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask, hidden_layer, num_classes\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "53a6faf2",
      "metadata": {
        "id": "53a6faf2"
      },
      "outputs": [],
      "source": [
        "os.makedirs('../ckpt', exist_ok=True)\n",
        "os.makedirs('../results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aa0b176f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa0b176f",
        "outputId": "9c73c400-3b20-4711-a9e6-0ae44b5c12b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "val_coco = COCO('../dataset/val_coco.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "gaSo0WXadCkN",
      "metadata": {
        "id": "gaSo0WXadCkN"
      },
      "outputs": [],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "809e1613",
      "metadata": {
        "id": "809e1613"
      },
      "outputs": [],
      "source": [
        "# model = build_model(4).to(device)\n",
        "# a =evaluate_model(model, val_loader, val_coco, 1, device)\n",
        "# print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d5ad386b",
      "metadata": {
        "id": "d5ad386b"
      },
      "outputs": [],
      "source": [
        "# do_coco_eval('/home/bhg/visual_dl/lab3/results/1_res.json', '../dataset/val_coco.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e358b9b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472,
          "referenced_widgets": [
            "0440fb6aae194f73b638a9c6b0b40a22",
            "d46e978269e346c8b28d3d19579eb87d",
            "955ea8e046f94720ad271acc774d2247",
            "7b46d55002eb436fa91e3fbf76e48d80",
            "6bac7740209b49a7a4c3297234d5e8b3",
            "24b19d54b47d415a9081e0f620d01267",
            "28953fbdcf7242a6871d9416ac31520f",
            "a49da35d13684d3f85a74196dbbf15b7",
            "388c6269bf92445b98087a0ccaeb42ca",
            "241a44ac29ac49c4b0dc798c3da4588a",
            "e7a2ce935ec249858276d0994b09559b",
            "de17b02bf170447a81d3955a02b72fea",
            "f42f3891d7b648df82f19ba33444da97",
            "4322fd8d4e8b44caabfa81f2067f6baa",
            "48f8c64c70794d1aa079530b4ef57c04",
            "c2efcfa29e5f46a88b65556dbb1a6e61",
            "473ed7951efe4b24ae583cf3151206dd",
            "5d93a6b3407a4307b14eec667c9e76a7",
            "7508630863b3451db5ebf5519b365f62",
            "17def1d4f80f4890aeffe67d7cf58807",
            "a859580950774dfabb7dd5f463771ea0",
            "6c3e7cf865d645158276de8ac39e48d9"
          ]
        },
        "id": "e358b9b3",
        "outputId": "91b8471a-6958-4d87-a3af-d24e480ba294"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0440fb6aae194f73b638a9c6b0b40a22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/42 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de17b02bf170447a81d3955a02b72fea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b2472cb226fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# print(img_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0msum_of_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss_per_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_of_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mregression_targets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regression_targets cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m             \u001b[0mloss_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_box_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfastrcnn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss_classifier\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss_box_reg\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_box_reg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mfastrcnn_loss\u001b[0;34m(class_logits, box_regression, labels, regression_targets)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# the corresponding ground truth labels, to be used with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# advanced indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msampled_pos_inds_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mlabels_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_pos_inds_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "model = build_model(num_classes).to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "# optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=0.0005)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "lr = 5e-4\n",
        "num_epochs = 1\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps=num_epochs*len(train_loader))\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    bar = tqdm(train_loader, desc=f\"Training\")\n",
        "\n",
        "    loss_per_epoch = []\n",
        "    # fd = False\n",
        "    for img_ids, images, targets in bar:\n",
        "        # if not fd:\n",
        "        #     show_image_with_boxes_and_masks(images[0], targets[0]['boxes'], targets[0]['masks'], 'None')\n",
        "        #     fd = True\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # print(targets[0])\n",
        "        # print(img_ids)\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        sum_of_loss = sum(loss for loss in loss_dict.values())\n",
        "        loss_per_epoch.append(sum_of_loss.detach().cpu().item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sum_of_loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        bar.set_postfix(loss=np.mean(loss_per_epoch))\n",
        "        bar.update()\n",
        "\n",
        "    # break\n",
        "\n",
        "    bar.close()\n",
        "\n",
        "    eval = evaluate_model(model, val_loader, val_coco, epoch, device)\n",
        "    # print(eval)\n",
        "    # with open(f'../results/{epoch}.txt', 'w') as f:\n",
        "    #     f.write(eval)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      torch.save(model.state_dict(), f'../ckpt/{epoch}.pth')\n",
        "\n",
        "torch.save(model.state_dict(), '../ckpt/last.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6ac3a4",
      "metadata": {
        "id": "0c6ac3a4"
      },
      "outputs": [],
      "source": [
        "test_transform=T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize(size=all_img_size, antialias=True),\n",
        "    # T.CenterCrop(size=224),\n",
        "    # T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_set = MedicalDataset(root_dir=test_dir, transform=test_transform, data_type='Test')\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKER, pin_memory=False, persistent_workers=True)\n",
        "\n",
        "# tt=T.ToPILImage()(test_set[0][1]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29538a47",
      "metadata": {
        "id": "29538a47"
      },
      "outputs": [],
      "source": [
        "ckpt_path = f'../ckpt/0.pth'\n",
        "model = build_model(num_classes).to(device)\n",
        "# model.load_state_dict(torch.load(ckpt_path))\n",
        "model.eval()\n",
        "\n",
        "res=[]\n",
        "\n",
        "for img_id, image in test_set:\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    # print(image.shape)\n",
        "    with torch.no_grad():\n",
        "        predictions = model(image)\n",
        "    # print(predictions)\n",
        "\n",
        "    # coco_results = []\n",
        "    # for img_id, prediction in zip([img_id], predictions):\n",
        "    #     for score, mask, label, box in zip(prediction['scores'], prediction['masks'], prediction['labels'], prediction['boxes']):\n",
        "    #         # print(mask.shape)\n",
        "    #         # rle = encode_mask(mask.detach().cpu().squeeze().numpy())\n",
        "    #         arr = np.asfortranarray(mask.detach().cpu()[0]).astype(np.uint8)\n",
        "    #         rle = mask_utils.encode(arr)\n",
        "    #         rle['counts'] = rle['counts'].decode('ascii')\n",
        "    #         coco_results.append({\n",
        "    #             \"image_id\": int(img_id),\n",
        "    #             \"bbox\": box.tolist(),\n",
        "    #             \"score\": score.item(),\n",
        "    #             \"category_id\": label.item(),\n",
        "    #             \"segmentation\": rle,\n",
        "    #         })\n",
        "    coco_results = predictions_to_coco([img_id], predictions, True)\n",
        "    res+=coco_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d103d92a",
      "metadata": {
        "id": "d103d92a"
      },
      "outputs": [],
      "source": [
        "print(res[1])\n",
        "with open('rere.json', 'w') as f:\n",
        "    json.dump(res, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2813e4ba",
      "metadata": {
        "id": "2813e4ba"
      },
      "outputs": [],
      "source": [
        "ckpt_path = f'../ckpt/0.pth'\n",
        "model = build_model(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(ckpt_path))\n",
        "model.eval()\n",
        "\n",
        "coco_results = []\n",
        "\n",
        "bar = tqdm(test_loader, desc=\"Inference\")\n",
        "for img_ids, images in bar:\n",
        "    images = list(image.to(device) for image in images)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(images)\n",
        "\n",
        "    batch_coco_results = predictions_to_coco(img_ids, predictions, True)\n",
        "    coco_results += batch_coco_results\n",
        "\n",
        "    bar.update()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7caea5d",
      "metadata": {
        "id": "f7caea5d"
      },
      "outputs": [],
      "source": [
        "print(coco_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd37e50f",
      "metadata": {
        "id": "cd37e50f"
      },
      "outputs": [],
      "source": [
        "with open(f'../submission.json', 'w') as f:\n",
        "    json.dump(coco_results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af49bc5b",
      "metadata": {
        "id": "af49bc5b"
      },
      "outputs": [],
      "source": [
        "with open(f'../submission.json', 'r') as f:\n",
        "    ff=json.load(f)\n",
        "with open(f'../dataset/test_image_name_to_ids.json', 'r') as f:\n",
        "    ee=json.load(f)\n",
        "\n",
        "ind = 3\n",
        "imgid = ff[ind]['image_id']\n",
        "box = torch.tensor(np.array(ff[ind]['bbox'])).unsqueeze(0)\n",
        "seg = ff[ind]['segmentation']\n",
        "img_path = ee[imgid-1]['file_name']\n",
        "img = Image.open(f'../dataset/test_release/{img_path}')\n",
        "img = T.ToTensor()(img)\n",
        "mask = torch.tensor(np.array(decode_maskobj(seg))).unsqueeze(0)\n",
        "# for i in mask:\n",
        "#     print(i)\n",
        "# plt.imshow(mask)\n",
        "plt.show()\n",
        "show_image_with_boxes_and_masks(img, box, mask, 'sjjs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c4e071",
      "metadata": {
        "id": "c7c4e071"
      },
      "outputs": [],
      "source": [
        "# model = torchvision.models.get_model(\n",
        "#         args.model, weights=args.weights, weights_backbone=args.weights_backbone, num_classes=num_classes, **kwargs\n",
        "#     )\n",
        "# model.roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=len(class_names),bias=True)\n",
        "# model.roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=len(class_names)*4,bias=True)\n",
        "# model.roi_heads.mask_predictor.mask_fcn_logits = nn.Conv2d(256, len(class_names),kernel_size=(1,1),stride=(1,1))\n",
        "\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06683b20",
      "metadata": {
        "id": "06683b20"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_loader = DataLoader(test_set, batch_size=2, shuffle=False)\n",
        "\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        outputs = model(batch.to(device))\n",
        "        results.extend(outputs)\n",
        "\n",
        "# 生成最終提交文件\n",
        "with open('test-results.json', 'w') as f:\n",
        "    json.dump(masks_to_coco(results, test_set.image_ids), f)\n",
        "\n",
        "print(\"Submission file generated!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lab6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0440fb6aae194f73b638a9c6b0b40a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d46e978269e346c8b28d3d19579eb87d",
              "IPY_MODEL_955ea8e046f94720ad271acc774d2247",
              "IPY_MODEL_7b46d55002eb436fa91e3fbf76e48d80"
            ],
            "layout": "IPY_MODEL_6bac7740209b49a7a4c3297234d5e8b3"
          }
        },
        "d46e978269e346c8b28d3d19579eb87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b19d54b47d415a9081e0f620d01267",
            "placeholder": "​",
            "style": "IPY_MODEL_28953fbdcf7242a6871d9416ac31520f",
            "value": "Epochs:   0%"
          }
        },
        "955ea8e046f94720ad271acc774d2247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a49da35d13684d3f85a74196dbbf15b7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_388c6269bf92445b98087a0ccaeb42ca",
            "value": 0
          }
        },
        "7b46d55002eb436fa91e3fbf76e48d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_241a44ac29ac49c4b0dc798c3da4588a",
            "placeholder": "​",
            "style": "IPY_MODEL_e7a2ce935ec249858276d0994b09559b",
            "value": " 0/1 [00:14&lt;?, ?it/s]"
          }
        },
        "6bac7740209b49a7a4c3297234d5e8b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b19d54b47d415a9081e0f620d01267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28953fbdcf7242a6871d9416ac31520f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a49da35d13684d3f85a74196dbbf15b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "388c6269bf92445b98087a0ccaeb42ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "241a44ac29ac49c4b0dc798c3da4588a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a2ce935ec249858276d0994b09559b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de17b02bf170447a81d3955a02b72fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f42f3891d7b648df82f19ba33444da97",
              "IPY_MODEL_4322fd8d4e8b44caabfa81f2067f6baa",
              "IPY_MODEL_48f8c64c70794d1aa079530b4ef57c04"
            ],
            "layout": "IPY_MODEL_c2efcfa29e5f46a88b65556dbb1a6e61"
          }
        },
        "f42f3891d7b648df82f19ba33444da97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473ed7951efe4b24ae583cf3151206dd",
            "placeholder": "​",
            "style": "IPY_MODEL_5d93a6b3407a4307b14eec667c9e76a7",
            "value": "Training:   0%"
          }
        },
        "4322fd8d4e8b44caabfa81f2067f6baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7508630863b3451db5ebf5519b365f62",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17def1d4f80f4890aeffe67d7cf58807",
            "value": 0
          }
        },
        "48f8c64c70794d1aa079530b4ef57c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a859580950774dfabb7dd5f463771ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_6c3e7cf865d645158276de8ac39e48d9",
            "value": " 0/42 [00:14&lt;?, ?it/s]"
          }
        },
        "c2efcfa29e5f46a88b65556dbb1a6e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473ed7951efe4b24ae583cf3151206dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d93a6b3407a4307b14eec667c9e76a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7508630863b3451db5ebf5519b365f62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17def1d4f80f4890aeffe67d7cf58807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a859580950774dfabb7dd5f463771ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c3e7cf865d645158276de8ac39e48d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}